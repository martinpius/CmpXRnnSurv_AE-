# -*- coding: utf-8 -*-
"""Complex_surv_loss_new

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-iuB1urUIP3xMKQ7WiWFyoP6J7_9Sn_8
"""

import torch
from torch import nn, mm



class COMPLEX_LOSS(nn.Module):
  """
  @Author: Martin Pius
  --------------------
  This class compute the complex loss for recurrent events
  with competing risks survival problem. Its combines 3 lossess
  namely negative log-likelihood, the ranking loss, and the huber loss

  Parameters:
  ------------
  a, b, c : Float values
  preds: torch.tensor with shape [batch_size, num_events, max_time_horizon]==> output from the network
  surv_time: torch.Tensor with shape [batch_size, 1] ==> ground truth survival times
  label: torch.Tensor with shape [batch_size, 1] ==> Labels for the events and censore
  AE_out: torch.Tensor with shape [batch_size, Ae_out_dim] ==> external autoencode's output
  input: torch.Tensor with shape [batch_size, input_dim] ==> original input to the external AE

  Returns:
  --------
  loss: torch.Tensor
  """
  def __init__(self, a, b, c):
    super(COMPLEX_LOSS, self).__init__()
    self.a = a
    self.b = b
    self.c = c
    self.hl = torch.nn.functional.huber_loss
  
  def forward(self,preds, surv_time, label, AE_out,input):
    l1 = likelihood_loss1(preds, surv_time, label)
    l2 = ranking_loss(preds, surv_time, label)
    l3 = self.hl(input, AE_out)
    l_total = self.a *l1 + self.b *l2 + self.c * l3
    return l_total

def my_logfn(x):
  """
  @Author: Martin Pius
  ----------------------
  This function avoid zero multiplication
  parameters:
  -----------
  x: torch.Tensor
  """
  return torch.log(x + 1e-8)

def my_div(x, y):
  """
  @Author: Martin Pius
  --------------------
  This function avoid division by zero
  parameters:
  -----------
  x: Torch.Tensor
  y: Torch.Tensor
  """
  return torch.div(x, (1e-8 + y))

def mask_ll(surv_time, label, num_events, max_time_horizon):
  """
  @Author: Martin Pius
  ---------------------
  This mask is necessary to compute the log-likelihood function for the complex survival model.
  
  Parameters:
  ------------
  surv_time: torch.Tensor ==> survival time with shape [batch_size, 1]
  num_events: Int ==> number of events 
  max_time_horizon: Int ==> maximum survival time accross all time steps
  label:torch.Tensor, indicator function: shape == [batch_size, 1]
  """
  batch_size = surv_time.shape[0]
  ll_mask = torch.zeros(size = (batch_size, num_events, max_time_horizon))
  for i in range(batch_size):
    if label[i,0] != 0: 
      ll_mask[i, int(label[i,0] - 1), int(surv_time[i,0])] = 1
    else: # instance is censored 
      ll_mask[i, :, int(surv_time[i,0] + 1):] = 1 # fills the rest with 1 until the censoring time
  return ll_mask
  
def mask_denominator(surv_time, num_events, max_time_horizon):
  """
  @Author: Martin Pius
  ---------------------

  This mask is suitable to compute the denominator of
  the log-likelihood loss. We fills with 1 until the last survival time
  
  Parameters:
  -----------
  surv_time: torch.Tensor, survival times with shape [batch_size, 1]
  num_eventa: Int, total number of event counts
  max_time_horizon: Int, maximum survival time accross all time steps
  
  Returns:
  mask_den: torch.Tensor
  """
  batch_size = surv_time.shape[0]
  mask_den = torch.zeros(size = (batch_size, num_events, max_time_horizon))
  for i in range(batch_size):
    mask_den[i, :, : int(surv_time[i, 0] + 1)] = 1
  return mask_den
  
def mask_logrank(surv_time, max_time_horizon):
  """
  @Author: Martin Pius
  ---------------------

  This mask is suitable to compute the ranking loss:
  ranking loss == we compare the probs of two individuals
  for a specified risk/event-type accross time stamps.

  Parameters:
  surv_time: torch.Tensor, survival times with shape [batch_size, 1]
  max_time_horizon: Int, maximum survival time accross time steps
  Returns:
  lmask_lr: torch.Tensor
  """
  batch_size = surv_time.shape[0]
  mask_lr = torch.zeros(size = (batch_size, max_time_horizon))
  for i in range(batch_size):
    t = int(surv_time[i, 0]) # grab the ith survival/censorship time
    mask_lr[i, :(t + 1)] = 1 # we fills with 1 from the start to the event time point
  return mask_lr



def ranking_loss(predictions, surv_time, label):
    """
    @Author: Martin Pius
    --------------------
    This module computes the ranking loss.The main purpose is to compare the tolerance of two
    individuals concerning the specified risk accross time steps.

    Parameters:
    -----------
    predictions: torch.Tensor, Network final predictions with shape [batch_size, num_events, max_time_horizon]
    surv_time: torch.Tensor, survival times with shape [batch_size, 1]
    label: torch.Tensor, indicator for the labels 0->censored, 1-> risk1, 2->risk2 etc: shape ==> [batch_size, 1]
    mask1: torch.Tensor, mask to compute the denominator of the log-likelihood function: shape ==> [batch_size, num_events, max_time_horizon]
    mask2: torch.Tensor, mask to compute the CIF for uncensored events: shape == [batch_size, num_events, max_time_horizon]

    Returns:
    loss_main: torch.Tensor

    """
    sigma1 = torch.tensor(data = 1, dtype = torch.float32)
    eps = 1e-8
    batch_size = surv_time.shape[0]
    max_time_horizon = predictions.shape[2]
    num_events = predictions.shape[1]
    ranking_mask = mask_logrank(surv_time, max_time_horizon) # shape ==  [batch_size, max_time_horizon]
    ranking_mask1 = torch.ones_like(ranking_mask)
    theta = [] # we compute the rank loss at every time step
    # ****rectify the shape for events indicator (whenever necessary) to suit computation ****
    for k in range(num_events):
      one_vec = torch.ones_like(surv_time, dtype = torch.float32)
      #events_indicator = torch.tensor(torch.eq(label, (k+1)), dtype = torch.float32)
      events_indicator = torch.eq(label, (k+1)).clone().detach().type(torch.float32).requires_grad_(True)
      #  the elements of the diagonal are the one we want (the rest of the elements are zeros)
      events_indicator = torch.diag(torch.squeeze(events_indicator)).clone().detach()
      # risk-specific joint probabilities ()
      tmp = predictions[:, k, :].unsqueeze(1)
      tmp = tmp.view(-1, tmp.shape[2]) # shape == [batch_size * num_events, max_time_horizon]
      # applying the mask
      ####### R = mm(tmp, torch.transpose(ranking_mask, 0, 1)) Hapaaa
      R = mm(tmp, torch.transpose(ranking_mask1, 0, 1))
      #R = torch.matmul(tmp, torch.transpose(ranking_mask, 0, 1)) # 2d * 2d == 2d tensor
      # combine the diagonals tensors and reshaping to 2d tensor shape like [batch_size, 1]
      #R_diag = torch.stack(tuple(t.diag() for t in torch.unbind(R,0))).view(-1, 1)
      ###########R_diag = torch.diag(R).view(R.shape[0], -1) # shape == [batch_size, 1] hapaa1
      R_diag1 = torch.diag(R).view(-1, 1)
      # we now take the difference of the paired instances 
      # for a specified risk (Rij = ri[Ti] - rj[Ti])==> (ith, and jth) individuals risk at time Ti
      # we multiply by a one_vec to extract the events
      ##### R = mm(one_vec, torch.transpose(R_diag1, 0,1)) - R HAPAAA 2
      R = mm(one_vec, torch.transpose(R_diag1, 0,1)) - R
      #R = torch.matmul(one_vec, torch.transpose(R_diag, 1, 0)) -  R
      R = torch.transpose(R, 0, 1) # transpose back to the original shape (it has some negative values we chope them by relu)
      T = nn.functional.relu6(torch.sign(mm(one_vec, torch.transpose(surv_time, 0, 1)) - mm(surv_time, torch.transpose(one_vec, 0, 1))))
      ##T = nn.functional.relu(torch.sign(mm(one_vec, torch.transpose(surv_time, 0, 1)) - mm(surv_time, torch.transpose(one_vec, 0, 1))))
      #T = nn.functional.relu(torch.sign(torch.matmul(one_vec, torch.transpose(surv_time, 0, 1)) - torch.matmul(surv_time, torch.transpose(one_vec, 1, 0))))
      T = mm(events_indicator, T)
      T = torch.clamp(T, eps, 1-eps)
      #T = torch.matmul(events_indicator, T) # retains the events only, Tij == 1 when event occured for subject i
      # compute the ranking loss according to the defined ranking loss formular
      tmp_theta = torch.mean(torch.clamp(T * torch.exp(-R / sigma1), eps, 1-eps), dim = 1, keepdim = True)
      theta.append(tmp_theta) # append the value to the container
      # stacking / combining the results on a new dim
    theta = torch.stack(theta, dim = 1)
    theta = torch.mean(theta.view(-1, num_events), dim = 1, keepdim = True)
    #theta = theta.clamp(0, 1)
    rnk_loss = torch.sum(theta) # adding the loss for all individuals in a batch
    return rnk_loss







def likelihood_loss1(predictions, surv_time, label):
    """
    @Author: Martin Pius
    ---------------------
    This module compute the negative log-likelihood loss for the recurrent events with competing
    risks survival problem.

    Parameters:
    ------------
    predictions: torch.Tensor, Network final predictions with shape [batch_size, num_events, max_time_horizon]
    surv_time: torch.Tensor, survival times with shape [batch_size, 1]
    label: torch.Tensor, indicator for the labels 0->censored, 1-> risk1, 2->risk2 etc: shape ==> [batch_size, 1]
    mask1: torch.Tensor, mask to compute the denominator of the log-likelihood function: shape ==> [batch_size, num_events, max_time_horizon]
    mask2: torch.Tensor, mask to compute the CIF for uncensored events: shape == [batch_size, num_events, max_time_horizon]

    Returns:
    loss_main: torch.Tensor
    """
    batch_size = predictions.shape[0]
    num_events = predictions.shape[1]
    max_time_horizon = predictions.shape[2]
    mask2 = mask_ll(surv_time, label, num_events, max_time_horizon)
    mask1 = mask_denominator(surv_time, num_events, max_time_horizon)
    indicator = torch.sign(label) # returns 1 for the events and 0 for the censores
    # we do element-wise multiplications of mask1 and the predictions and add accross time
    # the results is finally added accross number of events to obtain the CIF
    deno = 1 - torch.sum(torch.sum(mask1 * predictions, dim = 2), dim = 1, keepdim = True)
    # clip the values to avoid zero-division
    deno = torch.clip(deno.clone(), min = torch.tensor(1e-8, dtype = torch.float32), max = torch.tensor((1 - 1e-8), dtype = torch.float32))
    #deno.clamp(torch.tensor(1e-8, dtype = torch.float32), torch.tensor((1 - 1e-8), dtype = torch.float32))
    # the CIF for uncensored instances is computed using log-likelihood-mask2
    qt1 = torch.sum(torch.sum(mask2 * predictions, dim = 2), dim = 1, keepdim = True)
    #qt1 = torch.sum(torch.sum(mask2 * predictions, dim = 2), dim = 1, keepdim = True)
    qt1 = indicator * my_logfn(my_div(qt1, deno))
    # for the censored instances we compute the survival functions (opposite of cif)
    qt2 = (1 - indicator) * my_logfn(my_div(qt1, deno))
    tau = torch.tensor(data = 1.0, dtype = torch.float32)
    # compute the likelihood loss using the above computed components
    loss_main = -torch.mean(qt1 + tau * qt2)
    return loss_main



input = torch.randn(size = (8, 100))
ae_out = torch.randn(size = (8, 100))
#preds = torch.randint(low = 10, high = 100, size = (8, 12, 100), dtype = torch.float32)
preds = torch.randn(size = (8, 12, 100))
surv_time = torch.randint(low = 10, high = 100, size = (8,1), dtype = torch.float32)
label = torch.randint(low = 0, high = 3, size = (8,1))

l1 = likelihood_loss1(preds, surv_time, label)

l1

def hubber_loss1(targ, preds, delta = 1.0)->torch.Tensor:
  """
  @Author: Martin Pius
  --------------------
  This loss is necessary to train the external autoencoder to
  denoise the input data. This loss combines both mse-loss and
  l1-norm as regularizer.

  Parameters:
  ------------
  targ: torch.Tensor, shape ==> [batch_size, input_dim]
  preds: torch.Tensor, shape ==> [batch_size, input_dim]
  delta: Optional, Int: Hyperparameter for regularization

  Returns:
  ----------
  loss: torch.Tensor 
  """
  mse_huber = 0.5 * (targ - preds)**2
  mae_huber = delta * torch.abs(targ - preds) - 0.5 * delta**2
  loss = torch.where(0.5 * torch.abs(targ - preds) <= delta, mse_huber, mae_huber)
  return loss

##--------------x------------- sanity check for the loss function -------------x---------------##
input = torch.randn(size = (8, 100))
ae_out = torch.randn(size = (8, 100))
#preds = torch.randint(low = 10, high = 100, size = (8, 12, 100), dtype = torch.float32)
preds = torch.randn(size = (8, 12, 100))
surv_time = torch.randint(low = 10, high = 100, size = (8,1), dtype = torch.float32)
label = torch.randint(low = 0, high = 3, size = (8,1))

complex_loss = COMPLEX_LOSS(0.2, 1.6,1)
loss = complex_loss(preds*1.2 + 32, surv_time, label, ae_out, input)
print(f">>>> The loss is : {loss:.3f}")

