# -*- coding: utf-8 -*-
"""ComplexSurveHelper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uEIXW3ieRIZmlOSezEIsgIkI9I2e-On0
"""

from torch import nn, mm
import numpy as np
import pandas as pd
import os
import torch
from sklearn.preprocessing import MinMaxScaler

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pytorch-lightning

import pytorch_lightning as pl

class Features_Prep(pl.LightningModule):
  """
  @Author: Martin Pius
  ---------------------
  - This class preprocess the MIMIC3 data set to train the ComplexRNNSurv

  parameters:
  -------------
  categorical_nuniques_to_dims: Tupple of paired values for number of categories and embedding size
  num_numeric_features: Int, number of numeric features in the data
  fc_layers_construction: Tupple of Int values for the FC embedding layers
  dropout_probability: Int, dropout probability for regularization

  Returns:
  -------------
  out == rnn_inputs: Torch.Tensor, a preprocessed data matrix to train the ComplexRNNSurv

  """
  def __init__(self,
               categorical_nuniques_to_dims,
               num_numerical_features,
               fc_layers_construction,
               dropout_probability = 0.0):
    super(Features_Prep, self).__init__()
    self._make_embedding_layers(categorical_nuniques_to_dims)
    self._make_fc_layers(num_numerical_features, fc_layers_construction, dropout_probability)
  
  def _make_embedding_layers(self, nuniques_to_dims):
    """
    This method define the Embedding Layer for each categorical variable in the dataset.
    nunique_to_dim: This is the list of tupple in the form of key == (num_categories) -> value == (embedding size)
    self.embedding_layer_list : List of Embedding Layers applied to categorical variables
    self.num_categorical_features : Total number of categorical variables before Embedding
    self.num_embedded_features : Total number of embedded features for all categorical variables
    self.num_each_embedded_features : Number of embedded features for each categorical variable
    """
    self.num_categorical_features = len(nuniques_to_dims) # count the number of categorical features available
    self.num_embedded_features = 0
    self.num_each_embedded_features = []
    self.embedding_layer_list = nn.ModuleList() # a container to hold an embedding layer for each categorical feature
    for nunique_to_dim in nuniques_to_dims: # each key-value (as a pair) ==> single categorical variable
      num_uniques = nunique_to_dim[0] # grab the number of categories for a categorical variable ==  num-embeddings (in nn.Embedding)
      target_dim = nunique_to_dim[1] # grab embedding size for a specified categorical feature == embedding dims (in nn.Embedding)
      # we now construct embedding layers from categorical variables as:-
      self.embedding_layer_list.append(nn.Sequential(
                    nn.Embedding(num_uniques, target_dim),
                    #nn.BatchNorm1d(target_dim),
                    nn.ReLU(inplace=True)))
      self.num_embedded_features += target_dim # get the total number of embeddings for all categorical features
      self.num_each_embedded_features.append(target_dim) # get list of number of embeded_features per categorical variable
    
  def _make_fc_layers(self, num_numerical_features, fc_layers_construction, dropout_p):
    """
    -This method combines numeric features and the entity embeddings.
    -We further refines the features by passing through a FC-neural network to get the final input to our RNN
    => fc_layers_construction == list of desired output sizes correspoding to each embeding
    => self.fc_layer_list : List of Fully Connected Layers that take embedded categorical features and
    => numerical variables as inputs (to pre-process it----->refining the input features)
    => self.output_layer : Output layer -----> our input to the RNN
    """
    # input tensor to our RNN consists of categorical-entity embeddings and numeric features concatenated in such order.
    num_input = self.num_embedded_features + num_numerical_features # fc_input dimension (get the total number of input features)
    self.fc_layer_list = nn.ModuleList() # a container to hold fc_layer wrt each embedded layer
    # iterates over the number of outputs (desired outputs dims)
    for num_output in fc_layers_construction:
      self.fc_layer_list.append(nn.Sequential(
                    nn.Dropout(dropout_p) if dropout_p else nn.Sequential(),
                    nn.Linear(num_input, num_output),
                    #nn.BatchNorm1d(num_output),
                    nn.ReLU(inplace=True)))
      num_input = num_output
    self.output_layer = nn.Sequential(
            nn.Dropout(dropout_p) if dropout_p else nn.Sequential(),
            nn.Linear(num_input, num_input))
  
  def forward(self, input):
    """
    -input: shape == [batch_size, total number of features == (entity-embeddings + numeric features)]
    here we preprocess the combined features and return the inputs to our RNN
    if input == pd.Dataframe ---> batch_size/ total samples == len(input)
    if input == torch tensor ---> should be in shape [batch_size, num_features]

    """
    #Split the input into categorical variables and numerical variables
    # input == Pandas dataframe with the shape [num_rows, num_cols] 
    # later to be converted to torch tensor of shape ==> [batch_size , num_features]
    categorical_input = input[:, 0:self.num_categorical_features]
    numerical_input = input[:, self.num_categorical_features:]
    # Embed the categorical variables 
    embedded = torch.zeros(input.shape[0], self.num_embedded_features) # container for embeded features for each sample
    start_index = 0
    # loop through the embedded layers list for all categorical variables to build the entity embeddings
    for i, emb_layer in enumerate(self.embedding_layer_list):
      gorl_index = start_index + self.num_each_embedded_features[i]
      embedded[:, start_index:gorl_index] = emb_layer(categorical_input[:, i])
      start_index = gorl_index
    
    # Concatenate the embedded categorical features and the numerical variables and pass it to FC layers
    out = torch.cat([embedded, numerical_input], axis=1)
    return out # dim == [num_samples, num_features]



def normalize(dfm):
  """
  @Author: Martin Pius.
  ---------------------
  -This module normalizes the numeric variables

  Parameter:
  -----------
  dfm: pd.DataFrame with numerica variables only

  Returns:
  ----------
  dfm: pd.DataFrame which is normalized
  """
  x = dfm.values #returns a numpy array
  min_max_scaler = MinMaxScaler()
  x_scaled = min_max_scaler.fit_transform(x)
  dfm = pd.DataFrame(x_scaled)



