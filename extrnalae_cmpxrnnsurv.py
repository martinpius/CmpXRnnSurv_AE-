# -*- coding: utf-8 -*-
"""ExtrnalAE_CmpXrnnSurv

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16XSHtn9_X4KrOOHcGjaaF0d-bb7bWkeY
"""

!pip install  pytorch-lightning
import pytorch_lightning as pl
import torch
from torch import nn

class ExternalAE(pl.LightningModule):
  """
  @Author: Martin Pius
  #   ----------------------
  #   -This class implement the Inspired-Unsupervised Autoencoder for noise reduction

  #   Parameters:
  #   -----------
  #   input_dim: Int ==> dimension of the input data
  #   hidden_dim: Int ==> hidden dim for the FC net
  #   output_dim: Int ==> Embedding dimension

  #   Returns:
  #   ---------
  #   out: torch.Tensor--> Reconstructed input with shape [batch_size, input_dim]
  #   embedding: torch.Tensor--> The hidden representation with shape [batch_size, output_dim]

  """
  def __init__(self, 
               seq_len,
               cat_dim, 
               num_dim,
               hidden_num,
               hidden_cat,
               drp = 0.45):
    super(ExternalAE, self).__init__()
    
    self.seq_len = seq_len
    self.cat_dim = cat_dim
    self.num_dim = num_dim
    self.drp = drp
    self.hidden_num = hidden_num
    self.hidden_cat = hidden_cat
    self.Encoder1 = nn.Sequential(
        nn.Linear(self.num_dim, self.hidden_num),
        nn.BatchNorm1d(self.seq_len, affine = False),
        nn.ReLU(),
        nn.Sigmoid())
    self.Encoder2 = nn.Sequential(
        nn.Linear(self.cat_dim, self.hidden_cat),
        nn.Dropout(p = self.drp),
        nn.ReLU())
    self.Decoder1 = nn.Sequential(
        nn.Linear(self.hidden_num, self.num_dim),
        nn.ReLU())
    self.Decoder2 = nn.Sequential(
        nn.Linear(self.hidden_cat, self.cat_dim),
        nn.ReLU())
    
  def forward(self, input):
    batch_size = input.shape[0]
    x_cat = input[:,:,:self.cat_dim]
    wgn1 = torch.randn(size = (batch_size, self.seq_len, self.cat_dim))
    x1 = wgn1 + x_cat
    x_num = input[:,:,self.cat_dim:]
    wgn2 = torch.randn(size = (batch_size, self.seq_len, self.num_dim))
    x2 = wgn2 + x_num
    hd1 = self.Encoder1(x2)
    hd2 = self.Encoder2(x1)
    out = torch.cat([hd1,hd2], dim = 2)
    recon1 = self.Decoder1(hd1)
    recon2 = self.Decoder2(hd2)
    reconstruction = torch.cat([recon1, recon2], dim = 2)
    return reconstruction, out

