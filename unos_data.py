# -*- coding: utf-8 -*-
"""UNOS_DATA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SuXJnkXCA8jfghk524hnYO958p_jPQuw
"""

from google.colab import drive, files,auth
drive.mount("/content/drive/", force_remount = True)
try:
  COLAB = True
  from sklearn.preprocessing import MinMaxScaler, LabelEncoder
  import pandas as pd
  import os, gc
  import matplotlib.pyplot as plt
  import numpy as np
  from collections import Counter
  pd.set_option("max_rows", None)
  pd.set_option("max_columns", None)
  print(f">>>>: You are on CoLaB with Pandas version: {pd.__version__}")
  def timefmt(t: float)->float:
    h = int(t % 60)
    m = int(t % (60 * 60)/60)
    s = int(t % 60)
    return f"hrs: {h} min: {m:>02} sec: {s:>05.2f}"
except Exception as e:
  COLAB = False
  print(f"{type(e)}: {e}\n>>>> please correct {type(e)} and reload your drive")

#auth.authenticate_user()

# %%bigquery --project strange-song-274510 martin_unos
# select * 
# from  `strange-song-274510.UNOS.UNOS_FINALE`
# order by PT_CODE;

data_path = "/content/drive/MyDrive/MIMIC3_BIG_QUERY"
os.chdir(data_path)

def UNOS_PReP(data_name = "UNOS_FINALES.csv"):
  """
  @AUTHOR: Martin Pius
  ---------------------
  -This module preprocess the UNOS multi-recurrent events for ComplexSurv Model

  Arguments:
  --------
  data_name: str-Name of the pd.Dataset
  
  return:
  -------
  -Preprocessed Pandas DataFrame
  """
  print(f">>>> Loading the data: This may take a while, please waiting....")
  UNOS_data = pd.read_csv("UNOS_FINALES.csv", low_memory = False)
  print(f">>>> Pre-processing the data: This may take a while, please waiting....")
  UNOS_data["GSTATUS_PA"] = UNOS_data["GSTATUS_PA"].fillna(0) # To retain the feature during filtering 
  for col in UNOS_data.columns:
    if UNOS_data[col].isna().sum() > 100000:
      UNOS_data.drop(col, axis = 1, inplace= True)

  UNOS_data.drop(["DISCHARGE_DATE", "GTIME_KI", "TX_DATE"], axis = 1, inplace = True)
  UNOS_data["TRR_ID_CODE"] = UNOS_data["TRR_ID_CODE"].str.extract('(\d+)')
  for col in range(len(UNOS_data.columns)):
    if UNOS_data.iloc[:,col].dtype == object:
      UNOS_data.iloc[:,col] = UNOS_data.iloc[:,col].str.lstrip("b")

  for col in list(UNOS_data.columns):
    if UNOS_data.dtypes[col] == "object":
      UNOS_data[col] = UNOS_data[col].fillna("NA")
    else:
      UNOS_data[col] = UNOS_data[col].fillna(UNOS_data[col].mean())

  UNOS_data = UNOS_data.round(0)
  
  #Label creation
  label = []
  for k in range(len(UNOS_data)):
    if (UNOS_data.iloc[k,2] == 1) and (UNOS_data.iloc[k,4] == 0):
      label.append("KGRFT_Failure")
    elif (UNOS_data.iloc[k,3] == 0) and UNOS_data.iloc[k, 4] == 0:
      label.append("PGRFT_Failure")
    else:
      label.append("Censored")
  label = pd.Series(label)
  UNOS_data["label"] = label

  target = UNOS_data.loc[:, ["PT_CODE","TRR_ID_CODE","GSTATUS_KI","GSTATUS_PA","PSTATUS","label","PTIME"]]
  target_unos = target.loc[:, ["PT_CODE","TRR_ID_CODE","label", "PTIME"]]
  id_codes = UNOS_data.loc[:, ["PT_CODE","TRR_ID_CODE"]]
  cat_features = UNOS_data.loc[:, UNOS_data.dtypes == object]
  # Remove special characters from categorical features[map-apply method failed: we do it manually]
  cat_features["WL_ORG"] = cat_features["WL_ORG"].str.strip("''")
  cat_features["ON_DIALYSIS"] = cat_features["ON_DIALYSIS"].str.strip("''")
  cat_features["GENDER"] = cat_features["GENDER"].str.strip("''")
  cat_features["ABO"] = cat_features["ABO"].str.strip("''")
  cat_features["PERM_STATE"] = cat_features["PERM_STATE"].str.strip("''")
  cat_features["PERIP_VASC"] = cat_features["PERIP_VASC"].str.strip("''")
  cat_features["EXH_PERIT_ACCESS"] = cat_features["EXH_PERIT_ACCESS"].str.strip("''")
  cat_features["EXH_VASC_ACCESS"] = cat_features["EXH_VASC_ACCESS"].str.strip("''")
  cat_features["MALIG_TCR_KI"] = cat_features["MALIG_TCR_KI"].str.strip("''")
  cat_features["PREV_TX"] = cat_features["PREV_TX"].str.strip("''")
  cat_features["PREV_KI_TX"] = cat_features["PREV_KI_TX"].str.strip("''")
  cat_features["MALIG_TRR"] = cat_features["MALIG_TRR"].str.strip("''")
  cat_features["FIRST_WK_DIAL"] = cat_features["FIRST_WK_DIAL"].str.strip("''")
  cat_features["TXKID"] = cat_features["TXKID"].str.strip("''")
  cat_features["DON_RETYP"] = cat_features["DON_RETYP"].str.strip("''")
  cat_features["HBV_SUR_ANTIGEN_DON"] = cat_features["HBV_SUR_ANTIGEN_DON"].str.strip("''")
  cat_features["ABO_DON"] = cat_features["ABO_DON"].str.strip("''")
  cat_features["DON_TY"] = cat_features["DON_TY"].str.strip("''")
  cat_features["GENDER_DON"] = cat_features["GENDER_DON"].str.strip("''")
  cat_features["ABO_MAT"] = cat_features["ABO_MAT"].str.strip("''")
  cat_features["DIAL_TRR"] = cat_features["DIAL_TRR"].str.strip("''")
  cat_features["GRF_STAT_KI"] = cat_features["GRF_STAT_KI"].str.strip("''")
  cat_features["DWFG_KI"] = cat_features["DWFG_KI"].str.strip("''")
  cat_features["ORGAN"] = cat_features["ORGAN"].str.strip("''")
  cat_features["HBV_CORE"] = cat_features["HBV_CORE"].str.strip("''")
  cat_features["HBV_SUR_ANTIGEN"] = cat_features["HBV_SUR_ANTIGEN"].str.strip("''")
  cat_features["HCV_SEROSTATUS"] = cat_features["HCV_SEROSTATUS"].str.strip("''")
  cat_features["HIV_SEROSTATUS"] = cat_features["HIV_SEROSTATUS"].str.strip("''")
  cat_features["CMV_STATUS"] = cat_features["CMV_STATUS"].str.strip("''")
  cat_features["PREV_TX_ANY"] = cat_features["PREV_TX_ANY"].str.strip("''")
  cat_features["PX_STAT"] = cat_features["PX_STAT"].str.strip("''")
  cat_features["PAYBACK"] = cat_features["PAYBACK"].str.strip("''")
  cat_features["AGE_GROUP"] = cat_features["AGE_GROUP"].str.strip("''")
  cat_features["MALIG"] = cat_features["MALIG"].str.strip("''")
  cat_features["STATUS_TCR"] = cat_features["STATUS_TCR"].str.strip("''")
  cat_features["STATUS_TRR"] = cat_features["STATUS_TRR"].str.strip("''")
  cat_features["LT_ONE_WEEK_DON"] = cat_features["LT_ONE_WEEK_DON"].str.strip("''")
  cat_features["DATA_TRANSPLANT"] = cat_features["DATA_TRANSPLANT"].str.strip("''")
  cat_features["DATA_WAITLIST"] = cat_features["DATA_WAITLIST"].str.strip("''")

  cat_features.drop(["TRR_ID_CODE","INIT_DATE","END_DATE"], axis = 1, inplace = True)
  cat_feat_later = cat_features # To retain the original features for later vizualizations
  # Categorical encoding
  for col in list(cat_features):
    cat_features[col] = LabelEncoder().fit_transform(cat_features[col])
  num_features = UNOS_data.drop(cat_features, axis = 1)
  # Identify other categorical features from the numerical features
  cat_num = ['NUM_PREV_TX','CITIZENSHIP','EDUCATION','ETHCAT_DON','SHARE_TY',
           'DIAB','REM_CD','ETHCAT','REGION','PRI_PAYMENT_TCR_KI','DRMIS',
           'HLAMIS','CITIZENSHIP_DON']
  cat_num_features = num_features.loc[:, cat_num].astype('category')
  cat_features_unos = pd.concat([cat_num_features,cat_features], axis = 1)
  #numeric features
  num_features_unos = num_features.drop(cat_num, axis = 1)
  num_features_unos.drop(["INIT_DATE","END_DATE","PT_CODE","TRR_ID_CODE"], axis = 1, inplace = True)
  num_features_unos_cols = list(num_features_unos.columns)
  # Scaling numeric features
  num_features_unos_normalized = MinMaxScaler().fit_transform(num_features_unos.values)
  # re-creating pd.DataFrame for normalized features
  num_features_unos_normalized = pd.DataFrame(num_features_unos_normalized, columns = num_features_unos_cols)
  # Combining the final pre-processed data
  UNOS_CMPX_Surv_prep = pd.concat([id_codes, cat_features_unos, num_features_unos_normalized], axis = 1)
  UNOS_CMPX_Surv_unC = pd.concat([id_codes, cat_feat_later,num_features_unos], axis = 1)
  UNOS_CMPX_Surv_prep.drop(["GSTATUS_KI","GSTATUS_PA","PSTATUS"], axis = 1, inplace = True)
  unos_events_summary = pd.DataFrame({"Time_stamps": ["T1", "T2", "T3", "T4"],
                                    "KGRFT_Failure": [10180,4819,2160,904],
                                    "KGRFT_Censored": [14230,10084,5839,3410],
                                    "PGRFT_Failure": [11240, 5410,4118,196],
                                    "PGRFT_Censored": [8946,8021,4981,2512]})

  # extract only single occurence
  r1 = []
  list1 = list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).values())
  list2 = list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).keys())
  for k in range(len(list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).keys()))):
    if list1[k] == 1:
      r1.append(list2[k])
  rec1_idx = r1
  rec1_unos_prep = UNOS_CMPX_Surv_prep.loc[UNOS_CMPX_Surv_prep['PT_CODE'].isin(rec1_idx)]
  rec1_unos_surv_unc = UNOS_CMPX_Surv_unC.loc[UNOS_CMPX_Surv_unC['PT_CODE'].isin(rec1_idx)]

  # extract only 2 steps recurrents
  r2 = []
  list1 = list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).values())
  list2 = list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).keys())
  for k in range(len(list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).keys()))):
    if list1[k] == 2:
      r2.append(list2[k])
  rec2_idx = list(np.repeat(r2, 2))
  rec2_unos_prep = UNOS_CMPX_Surv_prep.loc[UNOS_CMPX_Surv_prep['PT_CODE'].isin(rec2_idx)]
  rec2_unos_surv_unc = UNOS_CMPX_Surv_unC.loc[UNOS_CMPX_Surv_unC['PT_CODE'].isin(rec2_idx)]
  rec2_target = target_unos.loc[target_unos['PT_CODE'].isin(rec2_idx)]

  # extract 3 steps recurrents
  r3 = []
  list1 = list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).values())
  list2 = list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).keys())
  for k in range(len(list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).keys()))):
    if list1[k] == 3:
      r3.append(list2[k])
  rec3_idx = list(np.repeat(r3, 3))
  rec3_unos_prep = UNOS_CMPX_Surv_prep.loc[UNOS_CMPX_Surv_prep['PT_CODE'].isin(rec3_idx)]
  rec3_unos_surv_unc = UNOS_CMPX_Surv_unC.loc[UNOS_CMPX_Surv_unC['PT_CODE'].isin(rec3_idx)]
  rec3_target = target_unos.loc[target_unos['PT_CODE'].isin(rec3_idx)]
  # extract 4 steps recurrents
  r4 = []
  list1 = list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).values())
  list2 = list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).keys())
  for k in range(len(list(Counter(UNOS_CMPX_Surv_prep.PT_CODE).keys()))):
    if list1[k] == 4:
      r4.append(list2[k])
  rec4_idx = list(np.repeat(r4, 4))
  rec4_unos_prep = UNOS_CMPX_Surv_prep.loc[UNOS_CMPX_Surv_prep['PT_CODE'].isin(rec4_idx)]
  rec4_unos_surv_unc = UNOS_CMPX_Surv_unC.loc[UNOS_CMPX_Surv_unC['PT_CODE'].isin(rec4_idx)]
  rec4_target = target_unos.loc[target_unos['PT_CODE'].isin(rec4_idx)]
  df_ = (target_unos, rec2_target, rec3_target,rec4_target,rec2_unos_prep, 
         rec2_unos_surv_unc,rec3_unos_prep, rec3_unos_surv_unc,
         rec4_unos_prep, rec4_unos_surv_unc)
  # padding the data with to match the shapes
  e3 = rec3_unos_prep.drop_duplicates(subset = 'PT_CODE')
  df_3 = pd.concat([rec3_unos_prep, e3], axis = 0)
  df_3 = df_3.set_index("PT_CODE").sort_index()

  e2 = rec2_unos_prep.drop_duplicates(subset = "PT_CODE")
  e2 = pd.concat([e2[:]]*2, ignore_index=True)
  df_2 = pd.concat([rec2_unos_prep, e2], axis = 0).set_index("PT_CODE").sort_index()

  # assembles all datafiles together
  unos_new = pd.concat([df_2, df_3,rec4_unos_prep], axis = 0)

  return unos_new, unos_events_summary, df_

