# -*- coding: utf-8 -*-
"""COMPLEX_SURVRnn_FIn

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qsI9wRbKk8aP34E3Z1HYthdT-wH9Klh9
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pytorch-lightning
# 
#

from torch import nn
import pytorch_lightning as pl
import time, os, torch
#import complex_surv_loss_new
from argparse import ArgumentTypeError

# class Encoder_RNN4Surv(pl.LightningModule):
#   '''
#   @Author: Martin Pius
#   --------------------
#   -The main rnn-module to be integrated with Lightning Module
#   -------------------------------------------------------------------------
#   external_ae: Optional, Bool==> default = True
#   rnn_type: list ==> ["RNN", "GRU", "LSTM"]
#   AE_dim: Optional, int ==> hidden size for external autoencoder
#   AE_out: Optional, int ==> embedding size for external autoencoder
#   --------------------------------------------------------------------------
#   '''

#   def __init__(self, 
#                input_dim,
#                hidden_dim,
#                num_layers,
#                rnn_type,
#                external_ae,
#                dropout,
#                AE_dim,
#                AE_out,
#                seq_len,
#                bidirectional = True):
    
#     super(Encoder_RNN4Surv, self).__init__()
#     self.input_dim = input_dim
#     self.hidden_dim = hidden_dim
#     self.num_layers = num_layers
#     self.external_ae = True
#     self.bidirectional = bidirectional
#     self.rnn_type = rnn_type
#     self.dropout = 0.25
#     self.AE_out = AE_out
#     self.AE_dim = AE_dim
#     self.seq_len = seq_len

#     if self.external_ae:
#       self.AE = ExternalAE(self.input_dim, self.AE_dim, self.AE_out)
#       self.input_dim = self.AE_out
#     else: self.input_dim = self.input_dim

#     self.rnn = nn.RNN(input_size = self.input_dim,
#                       hidden_size = self.hidden_dim,
#                       num_layers = self.num_layers,
#                       dropout = self.dropout,
#                       bidirectional = self.bidirectional)
    
#     self.gru = nn.GRU(input_size = self.input_dim,
#                       hidden_size = self.hidden_dim,
#                       num_layers = self.num_layers,
#                       dropout = self.dropout,
#                       bidirectional = self.bidirectional)
    
#     self.lstm = nn.LSTM(input_size = self.input_dim,
#                         hidden_size = self.hidden_dim,
#                         num_layers = self.num_layers,
#                         dropout = self.dropout,
#                         bidirectional = self.bidirectional)

#     if self.bidirectional:
#       self.fc_layer = nn.Linear(in_features =  2 * self.hidden_dim,
#                                 out_features =  self.hidden_dim)
#     else:
#       self.fc_layer = nn.Linear(in_features = self.num_layers * self.hidden_dim,
#                                 out_features = self.hidden_dim)
    

#   def forward(self, input):

#     '''
#     -%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%
#     input: torch.Tensor, This comes from the preprocessed data with the shape [batch_size, input_dim]
#     input: Reshaped ==> [seq_len, batch_size, input_dim]
#     h0, c0 : torch.Tensor, shape ==> [2 * num_layers, batch_size, hidden_dim] if BRNN
#            : shape ==> [num_layers, batch_size, hidden_dim] if RNN
#     out: torch.Tensor, shape ==> [batch_size, input_dim]
#     embedding: torch.Tensor, shape ==> [batch_size, AE_out]
#     rnn_out: torch.Tensor, shape ==> [batch_size, hidden_dim]
#     rnn_hidden: torch.Tensor, shape ==> [seq_len, batch_size, hidden_dim] if RNN
#                 shape ==> [seq_len, batch_size, 2 * hidden_dim] if BRNN 

#     -----------------------------------------------------------------------------------------------------
#     Returns: rnn_out : torch.Tensor, shape [seq_len, batch_size, hidden_dim] --> input to the attention network
#            : rnn_hidden: torch.Tensor, shape [batch_size, hidden_dim] --> input to the attention network

#     '''
#     if self.external_ae:
#       AE_out, embedding = self.AE(input)
#       #input = embedding.unsqueeze(0).repeat(self.seq_len, 1, 1)
#       input = embedding.reshape(embedding.shape[1], embedding.shape[0], embedding.shape[2])
#     else:
#       #input = input.unsqueeze(0).repeat(self.seq_len, 1, 1)
#       input = input.reshape(input.shape[1], input.shape[0], input.shape[2])
#     if self.bidirectional:
#       if self.rnn_type == "RNN":
#         h0 = torch.zeros(size = (2 * self.num_layers, input.shape[1], self.hidden_dim))
#         rnn_out, rnn_hidden = self.rnn(input, h0)
#         rnn_hidden = torch.cat([rnn_hidden[-2, :, :], rnn_hidden[-1, :, :]], dim = 1)
#         rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
#       elif self.rnn_type == "GRU":
#         h0 = torch.zeros(size = (2 * self.num_layers, input.shape[1], self.hidden_dim))
#         rnn_out, rnn_hidden = self.gru(input, h0)
#         rnn_hidden = torch.cat([rnn_hidden[-2, :, :], rnn_hidden[-1, :, :]], dim = 1)
#         rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
#       elif self.rnn_type == "LSTM":
#         h0 = torch.zeros(size = (2 * self.num_layers, input.shape[1], self.hidden_dim))
#         c0 = torch.zeros(size = (2 * self.num_layers, input.shape[1], self.hidden_dim))
#         rnn_out, (rnn_hidden, rnn_cell) = self.lstm(input, (h0, c0))
#         rnn_hidden = torch.cat([rnn_hidden[-2, :, :], rnn_hidden[-1, :, :]], dim = 1)
#         rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
#       else:
#         print(f">>>> {ArgumentTypeError}: rnn_type is either 'RNN', or 'LSTM', or 'GRU'")
#     else:
#       if self.rnn_type == "RNN":
#         h0 = torch.zeros(size = (self.num_layers, input.shape[1], self.hidden_dim))
#         rnn_out, rnn_hidden = self.rnn(input, h0)
#         rnn_hidden = rnn_hidden.view(rnn_hidden.shape[1],-1)
#         rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
#       elif self.rnn_type == "GRU":
#         h0 = torch.zeros(size = (self.num_layers, input.shape[1], self.hidden_dim))
#         rnn_out, rnn_hidden = self.gru(input, h0)
#         rnn_hidden = rnn_hidden.view(rnn_hidden.shape[1], -1)
#         rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
#       elif self.rnn_type == "LSTM":
#         h0 = torch.zeros(size = (self.num_layers, input.shape[1], self.hidden_dim))
#         c0 = torch.zeros(size = (self.num_layers, input.shape[1], self.hidden_dim))
#         rnn_out, (rnn_hidden, rnn_cell) = self.lstm(input, (h0, c0))
#         rnn_hidden = rnn_hidden.view(rnn_hidden.shape[1], -1)
#         rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
#       else:
#         print(f">>>> {ArgumentTypeError}: rnn_type is either 'RNN', or 'LSTM', or 'GRU'")
#     if self.external_ae:
#       return  rnn_out, rnn_hidden, embedding, AE_out
#     else:
#       return rnn_out, rnn_hidden



class Encoder_RNN4Surv(pl.LightningModule):
  '''
  @Author: Martin Pius
  --------------------
  -The main rnn-module to be integrated with Lightning Module
  -------------------------------------------------------------------------
  external_ae: Optional, Bool==> default = True
  rnn_type: list ==> ["RNN", "GRU", "LSTM"]
  AE_dim: Optional, int ==> hidden size for external autoencoder
  AE_out: Optional, int ==> embedding size for external autoencoder
  --------------------------------------------------------------------------
  '''

  def __init__(self, 
               input_dim,
               hidden_dim,
               num_layers,
               rnn_type,
               external_ae,
               dropout,
               seq_len,
               dim_num, 
               dim_cat,
               hidden_num,
               hidden_cat,
               bidirectional = True):
    
    super(Encoder_RNN4Surv, self).__init__()
    self.input_dim = input_dim
    self.hidden_dim = hidden_dim
    self.num_layers = num_layers
    self.external_ae = True
    self.bidirectional = bidirectional
    self.rnn_type = rnn_type
    self.dropout = 0.25
    self.dim_cat = dim_cat
    self.dim_num = dim_num
    self.hidden_num = hidden_num
    self.hidden_cat = hidden_cat
    self.seq_len = seq_len

    if self.external_ae:
      self.AE = ExternalAE(self.seq_len, self.dim_cat, self.dim_num, self.hidden_num, self.hidden_cat)
      self.AE_out = self.hidden_num + self.hidden_cat
      self.input_dim = self.AE_out
    else: self.input_dim = self.input_dim

    self.rnn = nn.RNN(input_size = self.input_dim,
                      hidden_size = self.hidden_dim,
                      num_layers = self.num_layers,
                      dropout = self.dropout,
                      bidirectional = self.bidirectional)
    
    self.gru = nn.GRU(input_size = self.input_dim,
                      hidden_size = self.hidden_dim,
                      num_layers = self.num_layers,
                      dropout = self.dropout,
                      bidirectional = self.bidirectional)
    
    self.lstm = nn.LSTM(input_size = self.input_dim,
                        hidden_size = self.hidden_dim,
                        num_layers = self.num_layers,
                        dropout = self.dropout,
                        bidirectional = self.bidirectional)

    if self.bidirectional:
      self.fc_layer = nn.Linear(in_features =  2 * self.hidden_dim,
                                out_features =  self.hidden_dim)
    else:
      self.fc_layer = nn.Linear(in_features = self.num_layers * self.hidden_dim,
                                out_features = self.hidden_dim)
    

  def forward(self, input):

    '''
    -%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%
    input: torch.Tensor, This comes from the preprocessed data with the shape [batch_size, input_dim]
    input: Reshaped ==> [seq_len, batch_size, input_dim]
    h0, c0 : torch.Tensor, shape ==> [2 * num_layers, batch_size, hidden_dim] if BRNN
           : shape ==> [num_layers, batch_size, hidden_dim] if RNN
    out: torch.Tensor, shape ==> [batch_size, input_dim]
    embedding: torch.Tensor, shape ==> [batch_size, AE_out]
    rnn_out: torch.Tensor, shape ==> [batch_size, hidden_dim]
    rnn_hidden: torch.Tensor, shape ==> [seq_len, batch_size, hidden_dim] if RNN
                shape ==> [seq_len, batch_size, 2 * hidden_dim] if BRNN 

    -----------------------------------------------------------------------------------------------------
    Returns: rnn_out : torch.Tensor, shape [seq_len, batch_size, hidden_dim] --> input to the attention network
           : rnn_hidden: torch.Tensor, shape [batch_size, hidden_dim] --> input to the attention network

    '''
    if self.external_ae:
      AE_out, embedding = self.AE(input)
      #input = embedding.unsqueeze(0).repeat(self.seq_len, 1, 1)
      input = embedding.reshape(embedding.shape[1], embedding.shape[0], embedding.shape[2])
    else:
      #input = input.unsqueeze(0).repeat(self.seq_len, 1, 1)
      input = input.reshape(input.shape[1], input.shape[0], input.shape[2])
    if self.bidirectional:
      if self.rnn_type == "RNN":
        h0 = torch.zeros(size = (2 * self.num_layers, input.shape[1], self.hidden_dim))
        rnn_out, rnn_hidden = self.rnn(input, h0)
        rnn_hidden = torch.cat([rnn_hidden[-2, :, :], rnn_hidden[-1, :, :]], dim = 1)
        rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
      elif self.rnn_type == "GRU":
        h0 = torch.zeros(size = (2 * self.num_layers, input.shape[1], self.hidden_dim))
        rnn_out, rnn_hidden = self.gru(input, h0)
        rnn_hidden = torch.cat([rnn_hidden[-2, :, :], rnn_hidden[-1, :, :]], dim = 1)
        rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
      elif self.rnn_type == "LSTM":
        h0 = torch.zeros(size = (2 * self.num_layers, input.shape[1], self.hidden_dim))
        c0 = torch.zeros(size = (2 * self.num_layers, input.shape[1], self.hidden_dim))
        rnn_out, (rnn_hidden, rnn_cell) = self.lstm(input, (h0, c0))
        rnn_hidden = torch.cat([rnn_hidden[-2, :, :], rnn_hidden[-1, :, :]], dim = 1)
        rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
      else:
        print(f">>>> {ArgumentTypeError}: rnn_type is either 'RNN', or 'LSTM', or 'GRU'")
    else:
      if self.rnn_type == "RNN":
        h0 = torch.zeros(size = (self.num_layers, input.shape[1], self.hidden_dim))
        rnn_out, rnn_hidden = self.rnn(input, h0)
        rnn_hidden = rnn_hidden.view(rnn_hidden.shape[1],-1)
        rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
      elif self.rnn_type == "GRU":
        h0 = torch.zeros(size = (self.num_layers, input.shape[1], self.hidden_dim))
        rnn_out, rnn_hidden = self.gru(input, h0)
        rnn_hidden = rnn_hidden.view(rnn_hidden.shape[1], -1)
        rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
      elif self.rnn_type == "LSTM":
        h0 = torch.zeros(size = (self.num_layers, input.shape[1], self.hidden_dim))
        c0 = torch.zeros(size = (self.num_layers, input.shape[1], self.hidden_dim))
        rnn_out, (rnn_hidden, rnn_cell) = self.lstm(input, (h0, c0))
        rnn_hidden = rnn_hidden.view(rnn_hidden.shape[1], -1)
        rnn_hidden = torch.tanh(self.fc_layer(rnn_hidden))
      else:
        print(f">>>> {ArgumentTypeError}: rnn_type is either 'RNN', or 'LSTM', or 'GRU'")
    if self.external_ae:
      return  rnn_out, rnn_hidden, embedding, AE_out
    else:
      return rnn_out, rnn_hidden



class __AdditiveAttentionV1__(pl.LightningModule):
  def __init__(self, rnn_hidden_dim, rnn_out_dim, bidirectional = True):
    """
    @Author: Martin Pius
    This method compute the weights (prabalities of feature's contribution)

    Parameters:
    -----------
    rnn_hidden_dim: Fetched from the return of the 
    rnn_encoder (hidden) which has the shape of [batch_size, hidden_dim]
    rnn_out_dim: Fetched from the the return of the rnn_encoder
    (rnn_output) which has the shape [seq_len, batch_size, output_dim]:
    -----------
    """
    super().__init__()
    if bidirectional:
      rnn_out_dim = rnn_out_dim + rnn_hidden_dim
    else: rnn_out_dim = rnn_hidden_dim
    self.attn = nn.Linear(rnn_out_dim, rnn_hidden_dim)
    self.v = nn.Linear(rnn_hidden_dim, 1, bias = False)
  
  def forward(self, rnn_hidden, rnn_output):
    seq_len = rnn_output.shape[0] 
    rnn_hidden = rnn_hidden.unsqueeze(1) # new_shape == [batch_size, 1, hidden_dim]
    rnn_hidden1 = rnn_hidden.repeat(1, seq_len, 1) # new shape == [batch_size, seq_len, hidden_dim]
    rnn_output = rnn_output.permute(1, 0, 2) # new shape == [batch_size, seq_len, output_dim]
    # We combine the rnn_out and hidden to be the input to the linear layer
    att_input = torch.cat((rnn_hidden1, rnn_output), dim = 2) # shape == [batch_size, seq_len, output_dim + hidden_dim]
    alphas = torch.tanh(self.attn(att_input))
    attention = self.v(alphas).squeeze()
    return nn.functional.softmax(attention, dim = 1)

# The top architecture consists of subnetworks of FC-Layers to predict survival time
# Risk-specific sub-networks (for each competing risk) takes the Context which is the 
# point-wise multiplication of RIW and original input tensor) produce the outputs 
# Predictions ==> Survival time distribution for the Risk-Specific-Networks
# Predictions shape is reshaped into ==> [batch_size, num_events, max_time_horizon]
class RiskSpecificSubNetworks(pl.LightningModule):
  def __init__(self, hidden_dim, 
               max_time_horizon,
               num_events, 
               attention, 
               dropout,
               input_dim,
               seq_len,
               rnn_output_dim):
    """
    hidden_dim: ==> hidden dimension for each FC-layers subnetwork
    max_time_horizon: ==> maximum survival time accross all competing events
    num_events: total number of competing events. 
    """
    super().__init__()
    self.input_dim = input_dim
    #self.surv_time_dim = surv_time_dim
    self.rnn_output_dim = rnn_output_dim
    self.attention = attention
    self.num_events = num_events
    self.seq_len = seq_len
    self.max_time_horizon = max_time_horizon
    # Container to hold nn-layers
    self.fc_list = nn.ModuleList([
        nn.Sequential(
          nn.Linear(self.input_dim*self.seq_len + self.rnn_output_dim, 2*hidden_dim),
          nn.PReLU(),
          nn.Dropout(dropout),
          nn.Linear(2*hidden_dim, hidden_dim),
          nn.Linear(hidden_dim, self.num_events * self.max_time_horizon),
          nn.Softmax(dim = 1)
        )
        for _ in range(self.num_events)])
    
  
  def forward(self, x, rnn_hidden,rnn_output):
    '''
    surv_time == survival time: shape == [batch_size, 1]
    x : rnn_input: shape == [batch, input_dim]
    rnn_output:shape == [seq_len, batch_size, output_dim]
    rnn_hidden: shape == [batch_size, hidden_dim]
    ### we need to build a context vector using the (RIW) ==> attention ####
    Note: RIW ==> attention: shape ==> [batch_size, seq_len]

    '''
    #torch.autograd.set_detect_anomaly(True) # for anomaly detection
    seq_len = rnn_output.shape[0]
    p = self.attention(rnn_hidden, rnn_output) # shape == [batch_size, seq_len]
    p = p.unsqueeze(1) # shape == [batch_size, 1, seq_len]
    rnn_out1 = rnn_output.permute(1,0,2) # new shape == [batch_size, seq_len, hidden*2]
    context = torch.bmm(p, rnn_out1) # shape == [1, batch_size, hidden *2]
    #context = torch.zeros(p.shape[0], p.shape[1], rnn_out1.shape[2]) # shape == [batch_size, 1, hidden_dim *2]
    #for i in range(rnn_out1.shape[0],2):
      #context[i] = torch.stack([rnn_out1[i] @ p[i], rnn_out1[i+1] @ p[i+1]])
    
    context = context.squeeze(1) # shape == [batch_size, hidden_dim*2]
    rsm_input = torch.cat((context, x.reshape(x.shape[0],-1)), dim = 1) #hapa rejea!!!!!!
    
    #rsm_input = torch.cat((x.unsqueeze(1).repeat(1,seq_len,1), context), dim = 2) # shape == [batch_size, 1, hidden *2 + input_dim]
    #rsm_input = rsm_input.view(rsm_input.shape[0], -1) # shape == [batch_size, hidden*2 + input_dim]
    a = []
    for i in range(self.num_events):
      a.append(rsm_input)
    predictions = torch.cat([fc(x) for x, fc in zip(a, self.fc_list)], dim = 1)
    return predictions.view(-1, self.num_events, self.max_time_horizon)
    #return context, rsm_input

class ComplexSurvNetwork(pl.LightningModule):
  """ 
  @Author: Martin Pius
  ----------------------
  This module combine the encoder and the decoder modules to create the final model

  parameters:
  -------------
  rnn_surv: The instance of encoder module
  risk_specific_nets: The instance of decoder module
  """
  def __init__(self, rnn_surv, risk_specific_nets):
    super(ComplexSurvNetwork, self).__init__()
    self. rnn_surv = rnn_surv
    self.risk_specific_nets = risk_specific_nets
    #self.device = device
  
  def forward(self, input):
    '''
    input: rnn_input with shape == [batch_size, input_dim] obtained from Features_Prep():
    predictions:final output with shape: [batch_size, num_events, max_time_horizon]
    '''
    #batch_size = input.shape[0]
    #num_events = target.shape[1]
    #max_time_horizon = target.shape[2]
    #predictions = torch.zeros(size = (batch_size, self.num_events, self.max_time_horizon)) # container to holds the predictions
    rnn_out, rnn_hidden,embedding, AE_out = self.rnn_surv(input)
    predictions = self.risk_specific_nets(input, rnn_hidden, rnn_out) # shape == [batch_size, num_events, max_time_horizon]
    #init_input = predictions[:, :,0] # grab the first survival time
    return predictions, AE_out # shapes == [batch_size, num_events, max_time_horizon], [batch_size, input_dim]

# class ExternalAE(pl.LightningModule):

#   """
#   @Author: Martin Pius
#   ----------------------
#   -This class implement the Inspired-Unsupervised Autoencoder for noise reduction

#   Parameters:
#   -----------
#   input_dim: Int ==> dimension of the input data
#   hidden_dim: Int ==> hidden dim for the FC net
#   output_dim: Int ==> Embedding dimension

#   Returns:
#   ---------
#   out: torch.Tensor--> Reconstructed input with shape [batch_size, input_dim]
#   embedding: torch.Tensor--> The hidden representation with shape [batch_size, output_dim]

#   """

#   def __init__(self, input_dim,
#                hidden_dim,
#                output_dim):
    
#     super(ExternalAE, self).__init__()
#     self.input_dim = input_dim
#     self.hidden_dim = hidden_dim
#     self.output_dim = output_dim

#     self.encoder = nn.Sequential(
#         nn.Linear(in_features = self.input_dim,
#                   out_features = self.hidden_dim),
#         nn.Sigmoid(),
#         nn.Linear(in_features = self.hidden_dim,
#                   out_features = self.output_dim),
#         nn.Sigmoid())
    
#     self.decoder = nn.Sequential(
#         nn.Linear(in_features = self.output_dim,
#                   out_features = self.hidden_dim),
#         nn.Sigmoid(),
#         nn.Linear(in_features = self.hidden_dim,
#                   out_features = self.input_dim),
#         nn.Sigmoid())
    
#   def forward(self, input):
#     embedding = self.encoder(input)
#     out = self.decoder(embedding)
#     return out, embedding

# # # ****** Sanity check for the model class ******* #  ****** Sanity check for the model class *******#
# hidden_fc = 512
# BATCH_SIZE = 16
# rnn_hidden_dim = 256
# max_time_horizon = 30
# num_events = 3
# num_layers = 2
# seq_len = 4
# dropout = 0.25
# input_dim = 55
# dim_cat = 25
# dim_num = 30
# hidden_cat = 10
# hidden_num = 18
# drp = 0.5
# surv_time_dim = 1
# rnn_type = ["RNN","GRU","LSTM"]

# rnn_input = torch.randn(size = (BATCH_SIZE, seq_len, input_dim))
# surv_time = torch.randn(size = (BATCH_SIZE, surv_time_dim))

# E = Encoder_RNN4Surv(input_dim,
#                rnn_hidden_dim,
#                num_layers,
#                rnn_type[2],
#                True,
#                dropout,
#                seq_len,
#                dim_num, 
#                dim_cat,
#                hidden_num,
#                hidden_cat,
#                bidirectional = True)


# # # sanity check for the encoder
# #encoder = Encoder_RNN4Surv(input_dim, rnn_hidden_dim, num_layers, rnn_type[2], True, 0.10,seq_len, cat_dim, num_dim, hidden_num,hidden_cat, False)
# rnn_output, rnn_hidden, embedding, AE_out = E(rnn_input)
# rnn_output_dim = rnn_output.shape[2]

# # # instantiate the attention mechanism
# attention = __AdditiveAttentionV1__(rnn_hidden_dim,rnn_output_dim)



# # # sanity check for the decoder
# decoder = RiskSpecificSubNetworks(hidden_fc,
#                                   max_time_horizon,
#                                   num_events,
#                                    attention,
#                                    dropout, input_dim,
#                                   seq_len,
#                                    rnn_output_dim)
# predictions = decoder(rnn_input, rnn_hidden,rnn_output)

# # # the final model
# model = ComplexSurvNetwork(E, decoder)
# predictions_final,AE_out  = model(rnn_input)
# assert predictions_final.shape == (BATCH_SIZE * num_events, num_events, max_time_horizon)
# assert AE_out.shape == (BATCH_SIZE, seq_len, input_dim)



class ExternalAE(pl.LightningModule):
  """
  @Author: Martin Pius
  #   ----------------------
  #   -This class implement the Inspired-Unsupervised Autoencoder for noise reduction

  #   Parameters:
  #   -----------
  #   input_dim: Int ==> dimension of the input data
  #   hidden_dim: Int ==> hidden dim for the FC net
  #   output_dim: Int ==> Embedding dimension

  #   Returns:
  #   ---------
  #   out: torch.Tensor--> Reconstructed input with shape [batch_size, input_dim]
  #   embedding: torch.Tensor--> The hidden representation with shape [batch_size, output_dim]

  """
  def __init__(self, 
               seq_len,
               cat_dim, 
               num_dim,
               hidden_num,
               hidden_cat,
               drp = 0.45):
    super(ExternalAE, self).__init__()
    
    self.seq_len = seq_len
    self.cat_dim = cat_dim
    self.num_dim = num_dim
    self.drp = drp
    self.hidden_num = hidden_num
    self.hidden_cat = hidden_cat
    self.Encoder1 = nn.Sequential(
        nn.Linear(self.num_dim, self.hidden_num),
        nn.BatchNorm1d(self.seq_len, affine = False),
        nn.ReLU(),
        nn.Sigmoid())
    self.Encoder2 = nn.Sequential(
        nn.Linear(self.cat_dim, self.hidden_cat),
        nn.Dropout(p = self.drp),
        nn.ReLU())
    self.Decoder1 = nn.Sequential(
        nn.Linear(self.hidden_num, self.num_dim),
        nn.ReLU())
    self.Decoder2 = nn.Sequential(
        nn.Linear(self.hidden_cat, self.cat_dim),
        nn.ReLU())
    
  def forward(self, input):
    batch_size = input.shape[0]
    x_cat = input[:,:,:self.cat_dim]
    wgn1 = torch.randn(size = (batch_size, self.seq_len, self.cat_dim))
    x1 = wgn1 + x_cat
    x_num = input[:,:,self.cat_dim:]
    wgn2 = torch.randn(size = (batch_size, self.seq_len, self.num_dim))
    x2 = wgn2 + x_num
    hd1 = self.Encoder1(x2)
    hd2 = self.Encoder2(x1)
    out = torch.cat([hd1,hd2], dim = 2)
    recon1 = self.Decoder1(hd1)
    recon2 = self.Decoder2(hd2)
    reconstruction = torch.cat([recon1, recon2], dim = 2)
    return reconstruction, out

