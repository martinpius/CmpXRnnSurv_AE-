# -*- coding: utf-8 -*-
"""CmpXRnnSurv_AE_DataLoader

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12-8WPyG7_k2Vg4g0OIw7TJaA6G_oCGV0
"""

from google.colab import drive, files, auth
drive.mount("/content/drive/", force_remount = True)
try:
  COLAB = True
  import pandas as pd
  import numpy as np
  import torch, time
  from collections import Counter
  from torch.utils.data import Dataset, DataLoader
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import LabelEncoder, MinMaxScaler
  import matplotlib.pyplot as plt
  import torch,os
  pd.set_option("display.max_columns", None)
  pd.set_option("display.max_rows", None)
  print(f">>>> You are on CoLaB with Pandas version {pd.__version__}")

  def __timefmt__(t: float = 123.178)->float:
    h = int(t / (60 * 60))
    m = int(t % (60 * 60) / 60)
    s = int(t % 60)
    return f"hrs: {h} min: {m:>02} sec: {s:>05.2f}"

except Exception as e:
  print(f">>>> {type(e)}: {e}\n>>>> Please corect {type(e)} and reload the drive")
  COLAB = False
print(f">>>> testing the time formating function...........\n>>>> time elapsed\t{__timefmt__()}")

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pytorch-lightning

files.upload()



import complex_survrnn_fin
import complexsurvehelper
import complexsurvsimulation
from mimic_iii_preprocessing import *
import mimic_iii_preprocessing
import complex_surv_eval_metric
import mimic3_loss, math
from torch import optim
from tqdm import tqdm
import unos_data_new
import complex_survrnn_fin_sim
import complexsurvrnn_msn



data_path = "/content/drive/MyDrive/MIMIC3_BIG_QUERY"
fnames = ['icu_clean1', 'icu_clean2', 'icu_clean3', "mimic3_clear_martin"]



# Commented out IPython magic to ensure Python compatibility.
# # Load and preprocess the mimic3-dataset
# %%capture
# print(f">>>> xxx-xxx-xxx-xxx-xxx-xxx...MIMIC3 DATA...xxx-xxx-xxx-xxx-xxx-xxx")
# tic = time.time()
# (label,label_tensor, label_dict),\
# (surv_time, surv_tensor, surv_time_dict),\
# (x, y), (cat_features, num_features),\
# (embedding_sizes, dfm,dfm1),\
# (df_1, df_2, df_3, df_4, data5) = MIMICIII_preprocess(data_path, fnames)
# 
# print(f">>>> xxx-xxx-xxx-xxx-xxx-xxx...UNOS-OPTN KIDPAN_DATA...xxx-xxx-xxx-xxx-xxx-xxx....")
# unos_new, unos_events_summary, df_  = unos_data_new.UNOS_PReP()
# 
# print(f">>>> xxxxxx-xxxxx-xxxxxx.....Synthetic Data....xxxx-xxxxx-xxxxx-xxxx-xxxxx")
# sim_data, events_summary, rec_dfms = complexsurvsimulation.__CreateCmpxRec__()
# toc = time.time()
#



print(f">>>> time elapsed: {__timefmt__(toc - tic)}")



def __mimic_summary__():

  """
  @Author: Martin Pius
  ---------------------
  This method compute the total number of events/censore at every time step

  argument:
  ---------
  pd.DataFrame: Takes dataframe at every recurrent step

  return
  -------
  pd.DataFrame
  """

  label1 = []
  for k in range(len(df_1)):
    if df_1.iloc[k,6] == 1 and df_1.iloc[k,-3] == 0:
      label1.append("heart problem")
    elif df_1.iloc[k,6] == 0 and df_1.iloc[k,-3] == 0:
      label1.append("other risks")
    elif df_1.iloc[k,6] == 0 and df_1.iloc[k,-3] == 1:
      label1.append("censored-heart")
    else:
      label1.append("censored-others")

  label2 = []
  for k in range(len(df_2)):
    if df_2.iloc[k,6] == 1 and df_2.iloc[k,-3] == 0:
      label2.append("heart problem")
    elif df_2.iloc[k,6] == 0 and df_2.iloc[k,-3] == 0:
      label2.append("other risks")
    elif df_2.iloc[k,6] == 0 and df_2.iloc[k,-3] == 1:
      label2.append("censored-heart")
    else:
      label2.append("censored-others")

  label3 = []
  for k in range(len(df_3)):
    if df_3.iloc[k,6] == 1 and df_3.iloc[k,-3] == 0:
      label1.append("heart problem")
    elif df_2.iloc[k,6] == 0 and df_3.iloc[k,-3] == 0:
      label3.append("other risks")
    elif df_3.iloc[k,6] == 0 and df_3.iloc[k,-3] == 1:
      label3.append("censored-heart")
    else:
      label3.append("censored-others")

  label4 = []
  for k in range(len(df_4)):
    if df_4.iloc[k,6] == 1 and df_4.iloc[k,-3] == 0:
      label4.append("heart problem")
    elif df_4.iloc[k,6] == 0 and df_4.iloc[k,-3] == 0:
      label4.append("other risks")
    elif df_4.iloc[k,6] == 0 and df_4.iloc[k,-3] == 1:
      label4.append("censored-heart")
    else:
      label4.append("censored-others")

  label5 = []
  for k in range(len(data5)):
    if data5.iloc[k,6] == 1 and data5.iloc[k,-3] == 0:
      label5.append("heart problem")
    elif data5.iloc[k,6] == 0 and data5.iloc[k,-3] == 0:
      label5.append("other risks")
    elif data5.iloc[k,6] == 0 and data5.iloc[k,-3] == 1:
      label5.append("censored-heart")
    else:
      label5.append("censored-others")

  E1 = pd.DataFrame(Counter(label1), index = [0])
  E2 = pd.DataFrame(Counter(label2), index = [0])
  E3 = pd.DataFrame(Counter(label3), index = [0])
  E4 = pd.DataFrame(Counter(label4), index = [0])
  E5 = pd.DataFrame(Counter(label5), index = [0])
  E1 = E1.iloc[:,[1,2,0,3]]
  E2 = E2.iloc[:,[3,1,0,2]]
  E3["heart problem"] = 0
  E3 = E3.iloc[:, [3,2,0,1]]
  E4 = E4.iloc[:, [0,3,1,2]]
  E5 = E5.iloc[:, [0,2,1,3]]
  E = pd.concat([E1, E2, E3, E4, E5], axis = 0)
  E["stamps"] = ["T1", "T2","T3","T4","T5"]

  event_counts = pd.DataFrame(data = {"stamps":["T1", "T2","T3","T4","T5"], 
                           "heart_condition": [5675,4461,1961,2204,1149],
                           "censored_heart":[1183,61,409,533,204],
                           "other_risks": [2155,1836,502,1036,895],
                           "censored_other":[187,154,3,327,809]})


  return event_counts

mimic3_events_summary = __mimic_summary__()



# Get the features and labels for synthetized data
covariates_sim = sim_data.set_index("subject_id").iloc[:,5:]
label_sim = sim_data.set_index("subject_id").iloc[:,6]
surv_time_sim = sim_data.set_index("subject_id").iloc[:,5]



unos_new.drop(["TRR_ID_CODE", "PT_CODE"], axis = 1, inplace = True)



# Get the features and labels for the UNOS-OPTN-KIDPAN data
label_un = unos_new.iloc[:, 52]
surv_time_un = unos_new.iloc[:, 53]
covariates_un = unos_new



# plotting summaries for the mimic3-data

colors = ['#5cb85c', '#5bc0de','#76EE00','#006400']
p1 = mimic3_events_summary.set_index("stamps").plot.bar(color=colors, figsize=(18, 10), ylabel='Counts', title="Event status distribution for MIMICIII dataset")
p1.set_xticklabels(p1.get_xticklabels(), rotation=0)

for p in p1.patches:
    p1.annotate(f'{p.get_height():0.0f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

#plt.savefig("myfigmimic.png")



colors = ['#5cb85c', '#5bc0de','#76EE00','#006400']
p1 = unos_events_summary.set_index("Time_stamps").plot.bar(color=colors, figsize=(18, 10), ylabel='Counts', title="Event status distribution for UNOS dataset")
p1.set_xticklabels(p1.get_xticklabels(), rotation=0)

for p in p1.patches:
    p1.annotate(f'{p.get_height():0.0f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

plt.savefig("myfigunos.png")



# Ploting summary events for the simulated dataset

colors = ['#5cb85c', '#5bc0de','#76EE00','#006400']
p1 = events_summary.set_index("stamps").plot.bar(color=colors, figsize=(18, 10), ylabel='Counts', title="Event status distribution for simulated data")
p1.set_xticklabels(p1.get_xticklabels(), rotation=0)


for patch in p1.patches:
    bl = patch.get_xy()
    x = 0.5 * patch.get_width() + bl[0]
    # change 0.92 to move the text up and down
    y = 0.92 * patch.get_height() + bl[1] 
    p1.text(x,y,"%d" %(patch.get_height()),
            ha='center', rotation='vertical', weight = 'bold')
p1.legend(loc = 'lower right')

#plt.savefig("simulated_rec.png")



# split the data into train-validation-test sets (60%-20%-20%)
def __train_test_split__(dataset_name, covariates, labels, surv_time):
  """
  @Author: Martin Pius
  ---------------------
  This method split the data into 80%-20% main-test and further
  splits the main data into 80% train 20% validation

  Arguments:
  ----------
  covariates: features values-pd.DataFrame
  s_time: survival time: pd.Series
  events_label: pd.Series

  returns:
  pd.DataFrame: Tupple
  """
  if dataset_name == "mimiciii_clinical":
    covariates["label"] = labels
    covariates['surv_time'] = surv_time
    x_main, x_test = covariates.iloc[:int(0.8 * len(covariates)),:], covariates.iloc[int(0.8 *len(covariates)):,:]
    x_train, x_val = x_main.iloc[:int(0.8 * len(x_main)),:], x_main.iloc[int(0.8 * len(x_main)):,:]
    x_test = x_test.iloc[4:,:]
    x_val = x_val.iloc[4:,:]
    time_main, time_test = surv_time[:int(0.8 * len(surv_time))], surv_time[int(0.8 *len(surv_time)):]
    label_main, label_test = labels[:int(0.8 * len(labels))], labels[int(0.8 *len(labels)):]
    time_train, time_val = time_main[:int(0.8 * len(time_main))], time_main[int(0.8 * len(time_main)):]
    label_train, label_val = label_main[:int(0.8 * len(label_main))], label_main[int(0.8 * len(label_main)):]
    time_test = time_test[4:]
    label_test = label_test[4:]
    time_val = time_val[4:]
    label_val = label_val[4:]
  elif dataset_name  == "simulated_cmpx":
       #covariates["label"] = labels
       #covariates = covariates.drop(['surv_time'], axis = 1)
       x_main, x_test = covariates.iloc[:int(0.8 * len(covariates)),:], covariates.iloc[int(0.8 *len(covariates)):,:]
       x_train, x_val = x_main.iloc[:int(0.8 * len(x_main)),:], x_main.iloc[int(0.8 * len(x_main)):,:]
       x_test = x_test.iloc[5:,:]
       x_val = x_val.iloc[5:,:]
       time_main, time_test = surv_time[:int(0.8 * len(surv_time))], surv_time[int(0.8 *len(surv_time)):]
       label_main, label_test = labels[:int(0.8 * len(labels))], labels[int(0.8 *len(labels)):]
       time_train, time_val = time_main[:int(0.8 * len(time_main))], time_main[int(0.8 * len(time_main)):]
       label_train, label_val = label_main[:int(0.8 * len(label_main))], label_main[int(0.8 * len(label_main)):]
       time_test = time_test[5:]
       label_test = label_test[5:]
       time_val = time_val[5:]
       label_val = label_val[5:]
  elif dataset_name == "unos_optn_kidpan":
    x_main, x_test = covariates.iloc[:int(0.8 * len(covariates)),:], covariates.iloc[int(0.8 *len(covariates)):,:]
    x_train, x_val = x_main.iloc[:int(0.8 * len(x_main)),:], x_main.iloc[int(0.8 * len(x_main)):,:]
    #x_test = x_test.iloc[5:,:]
    x_val = x_val.iloc[1:,:]
    time_main, time_test = surv_time.iloc[:int(0.8 * len(surv_time))], surv_time.iloc[int(0.8 *len(surv_time)):]
    label_main, label_test = labels.iloc[:int(0.8 * len(labels))], labels.iloc[int(0.8 *len(labels)):]
    time_train, time_val = time_main.iloc[:int(0.8 * len(time_main))], time_main.iloc[int(0.8 * len(time_main)):]
    label_train, label_val = label_main.iloc[:int(0.8 * len(label_main))], label_main.iloc[int(0.8 * len(label_main)):]
    #time_test = time_test[5:]
    #label_test = label_test[5:]
    time_val = time_val.iloc[1:]
    label_val = label_val.iloc[1:]

  else: print(f"Unsurported data file")

  return x_train, x_val, x_test, time_train, time_val, time_test, label_train, label_val, label_test



X_train_mimic, X_val_mimic, X_test_mimic,\
 s_time_train_mimic, s_time_val_mimic, \
 s_time_test_mimic, label_train_mimic, \
 label_val_mimic, label_test_mimic = __train_test_split__('mimiciii_clinical',dfm,label, surv_time)



X_train_unos, X_val_unos, X_test_unos,\
 s_time_train_unos, s_time_val_unos, \
 s_time_test_unos, label_train_unos, \
 label_val_unos, label_test_unos = __train_test_split__('unos_optn_kidpan',covariates_un,label_un, surv_time_un)



X_train_syt, X_val_syt, X_test_syt,\
 s_time_train_syt, s_time_val_syt, \
 s_time_test_syt, label_train_syt, \
 label_val_syt, label_test_syt = __train_test_split__('simulated_cmpx',covariates_sim,label_sim, surv_time_sim)



#X_train_syt.head(16)

print(f">>>> X_train_mimic3 shape: {X_train_mimic.shape}, X_val_mimic3 shape: {X_val_mimic.shape}, X_test_mimic3 shape: {X_test_mimic.shape}\n>>>> \
Y_train_mimic3 shape: {s_time_train_mimic.shape}, Y_val shape_mimic3: {s_time_val_mimic.shape}, Y_test_mimic3 shape: {s_time_test_mimic.shape}")

print(f">>>> X_train_UNOS shape: {X_train_unos.shape}, X_val_UNOS shape: {X_val_unos.shape}, X_test_UNOS shape: {X_test_unos.shape}\n>>>> \
Y_train_UNOS shape: {s_time_train_unos.shape}, Y_val shape_UNOS: {s_time_val_unos.shape}, Y_test_UNOS shape: {s_time_test_unos.shape}")

print(f">>>> X_train_syt shape: {X_train_syt.shape}, X_val_syt shape: {X_val_syt.shape}, X_test-syt shape: {X_test_syt.shape}\n>>>> \
Y_train_syt shape: {s_time_train_syt.shape}, Y_val-syt shape: {s_time_val_syt.shape}, Y_test_syt shape: {s_time_test_syt.shape}")



# Prepare the data for traing our complexRNNSurv model.

# # MIMICIII_clinical dataset.[Note that the sequence length is 5, we use patient's previous (4) info to predict the future (fifth).
# steps = 4
# nt_recs_mimic = len(X_train_mimic)
# n_cols = 38
# train_cv_mimic, train_tm_mimic, train_lb_mimic = [], [], []
# for i in range(4, nt_recs_mimic):
#   train_cv_mimic.append(X_train_mimic[i-4:i])
#   train_tm_mimic.append(s_time_train_mimic.iloc[i])
#   train_lb_mimic.append(pd.Series(label_train_mimic).iloc[i])

# nv_recs_mimic = len(X_val_mimic)
# n_cols = 38
# val_cv_mimic, val_tm_mimic, val_lb_mimic = [], [], []
# for i in range(4, nv_recs_mimic):
#   val_cv_mimic.append(X_val_mimic[i-4:i])
#   val_tm_mimic.append(s_time_val_mimic.iloc[i])
#   val_lb_mimic.append(pd.Series(label_val_mimic).iloc[i])

# nte_recs_mimic = len(X_test_mimic)
# n_cols = 38
# test_cv_mimic, test_tm_mimic, test_lb_mimic = [], [], []
# for i in range(4, nte_recs_mimic):
#   test_cv_mimic.append(X_test_mimic[i-4:i])
#   test_tm_mimic.append(s_time_test_mimic.iloc[i])
#   test_lb_mimic.append(pd.Series(label_test_mimic).iloc[i])

# nt_recs_syt = len(X_train_syt)
# n_cols = 14
# train_cv_syt, train_tm_syt, train_lb_syt = [], [], []
# for i in range(4, nt_recs_syt):
#   train_cv_syt.append(X_train_syt[i-4:i])
#   train_tm_syt.append(s_time_train_syt.iloc[i])
#   train_lb_syt.append(pd.Series(label_train_syt).iloc[i])

# nv_recs_syt = len(X_val_syt)
# n_cols = 14
# val_cv_syt, val_tm_syt, val_lb_syt = [], [], []
# for i in range(4, nv_recs_syt):
#   val_cv_syt.append(X_val_syt[i-4:i])
#   val_tm_syt.append(s_time_val_syt.iloc[i])
#   val_lb_syt.append(pd.Series(label_val_syt).iloc[i])

# nte_recs_syt = len(X_test_syt)
# n_cols = 14
# test_cv_syt, test_tm_syt, test_lb_syt = [], [], []
# for i in range(4, nte_recs_syt):
#   test_cv_syt.append(X_test_syt[i-4:i])
#   test_tm_syt.append(s_time_test_syt.iloc[i])
#   test_lb_syt.append(pd.Series(label_test_syt).iloc[i])


# x_tr_mimic, t_tr_mimic, l_tr_mimic = np.array(train_cv_mimic), np.array(train_tm_mimic), np.array(train_lb_mimic)
# x_vl_mimic, t_vl_mimic, l_vl_mimic = np.array(val_cv_mimic), np.array(val_tm_mimic), np.array(test_lb_mimic)
# x_te_mimic, t_te_mimic, l_te_mimic = np.array(test_cv_mimic), np.array(test_tm_mimic), np.array(test_lb_mimic)

# x_tr_syt, t_tr_syt, l_tr_syt = np.array(train_cv_syt), np.array(train_tm_syt), np.array(train_lb_syt)
# x_vl_syt, t_vl_syt, l_vl_syt = np.array(val_cv_syt), np.array(val_tm_syt), np.array(test_lb_syt)
# x_te_syt, t_te_syt, l_te_syt = np.array(test_cv_syt), np.array(test_tm_syt), np.array(test_lb_syt)



def __rnnDataFormat__():
  """
  @Author: Martin Pius
  --------------------
  This module prepares the final dataset to be fed into our RNN system
  Argument: 
  ---------
  df_names: List of strings, contain the dataset names
  df_list: List of pd.DataFrames

  returns:
  --------
  Tupple of np.ndarrays containing [training, validation and testing datasets]

  """
  print(f">>>> This may take a while!!! Please wait.....")
  steps = 4
  nt_recs_mimic = len(X_train_mimic)
  n_cols = 38
  train_cv_mimic, train_tm_mimic, train_lb_mimic = [], [], []
  for i in range(4, nt_recs_mimic):
    train_cv_mimic.append(X_train_mimic[i-4:i])
    train_tm_mimic.append(s_time_train_mimic.iloc[i])
    train_lb_mimic.append(pd.Series(label_train_mimic).iloc[i])

  nv_recs_mimic = len(X_val_mimic)
  n_cols = 38
  val_cv_mimic, val_tm_mimic, val_lb_mimic = [], [], []
  for i in range(4, nv_recs_mimic):
    val_cv_mimic.append(X_val_mimic[i-4:i])
    val_tm_mimic.append(s_time_val_mimic.iloc[i])
    val_lb_mimic.append(pd.Series(label_val_mimic).iloc[i])

  nte_recs_mimic = len(X_test_mimic)
  n_cols = 38
  test_cv_mimic, test_tm_mimic, test_lb_mimic = [], [], []
  for i in range(4, nte_recs_mimic):
    test_cv_mimic.append(X_test_mimic[i-4:i])
    test_tm_mimic.append(s_time_test_mimic.iloc[i])
    test_lb_mimic.append(pd.Series(label_test_mimic).iloc[i])

  nt_recs_syt = len(X_train_syt)
  n_cols = 14
  train_cv_syt, train_tm_syt, train_lb_syt = [], [], []
  for i in range(4, nt_recs_syt):
    train_cv_syt.append(X_train_syt[i-4:i])
    train_tm_syt.append(s_time_train_syt.iloc[i])
    train_lb_syt.append(pd.Series(label_train_syt).iloc[i])

  nv_recs_syt = len(X_val_syt)
  n_cols = 14
  val_cv_syt, val_tm_syt, val_lb_syt = [], [], []
  for i in range(4, nv_recs_syt):
    val_cv_syt.append(X_val_syt[i-4:i])
    val_tm_syt.append(s_time_val_syt.iloc[i])
    val_lb_syt.append(pd.Series(label_val_syt).iloc[i])

  nte_recs_syt = len(X_test_syt)
  n_cols = 14
  test_cv_syt, test_tm_syt, test_lb_syt = [], [], []
  for i in range(4, nte_recs_syt):
    test_cv_syt.append(X_test_syt[i-4:i])
    test_tm_syt.append(s_time_test_syt.iloc[i])
    test_lb_syt.append(pd.Series(label_test_syt).iloc[i])

  steps = 3
  nt_recs_unos = len(X_train_unos)
  n_cols = 111
  train_cv_unos, train_tm_unos, train_lb_unos = [], [], []
  for i in range(3, nt_recs_unos):
    train_cv_unos.append(X_train_unos.iloc[i-3:i,:])
    train_tm_unos.append(s_time_train_unos.iloc[i])
    train_lb_unos.append(label_train_unos.iloc[i])

  nv_recs_unos = len(X_val_unos)
  n_cols = 111
  val_cv_unos, val_tm_unos, val_lb_unos = [], [], []
  for i in range(3, nv_recs_unos):
    val_cv_unos.append(X_val_unos.iloc[i-3:i,:])
    val_tm_unos.append(s_time_val_unos.iloc[i])
    val_lb_unos.append(label_val_unos.iloc[i])

  nte_recs_unos = len(X_test_unos)
  n_cols = 111
  test_cv_unos, test_tm_unos, test_lb_unos = [], [], []
  for i in range(3, nte_recs_unos):
    test_cv_unos.append(X_test_unos.iloc[i-3:i,:])
    test_tm_unos.append(s_time_test_unos.iloc[i])
    test_lb_unos.append(label_test_unos.iloc[i])


  x_tr_mimic, t_tr_mimic, l_tr_mimic = np.array(train_cv_mimic), np.array(train_tm_mimic), np.array(train_lb_mimic)
  x_vl_mimic, t_vl_mimic, l_vl_mimic = np.array(val_cv_mimic), np.array(val_tm_mimic), np.array(test_lb_mimic)
  x_te_mimic, t_te_mimic, l_te_mimic = np.array(test_cv_mimic), np.array(test_tm_mimic), np.array(test_lb_mimic)

  x_tr_unos, t_tr_unos, l_tr_unos = np.array(train_cv_unos), np.array(train_tm_unos), np.array(train_lb_unos)
  x_vl_unos, t_vl_unos, l_vl_unos = np.array(val_cv_unos), np.array(val_tm_unos), np.array(test_lb_unos)
  x_te_unos, t_te_unos, l_te_unos = np.array(test_cv_unos), np.array(test_tm_unos), np.array(test_lb_unos)

  x_tr_syt, t_tr_syt, l_tr_syt = np.array(train_cv_syt), np.array(train_tm_syt), np.array(train_lb_syt)
  x_vl_syt, t_vl_syt, l_vl_syt = np.array(val_cv_syt), np.array(val_tm_syt), np.array(test_lb_syt)
  x_te_syt, t_te_syt, l_te_syt = np.array(test_cv_syt), np.array(test_tm_syt), np.array(test_lb_syt)
    
  mimic_train_data = (x_tr_mimic, t_tr_mimic, l_tr_mimic)
  mimic_validation_data = (x_vl_mimic, t_vl_mimic, l_vl_mimic)
  mimic_test_data = (x_te_mimic, t_te_mimic, l_te_mimic)

  unos_train_data = (x_tr_unos, t_tr_unos, l_tr_unos)
  unos_validation_data = (x_vl_unos, t_vl_unos, l_vl_unos)
  unos_test_data = (x_te_unos, t_te_unos, l_te_unos)

  syt_train_data = (x_tr_syt, t_tr_syt, l_tr_syt)
  syt_validation_data = (x_vl_syt, t_vl_syt, l_vl_syt)
  syt_test_data = (x_te_syt, t_te_syt, l_te_syt)

  return mimic_train_data, mimic_validation_data, mimic_test_data,unos_train_data, unos_validation_data, unos_test_data, syt_train_data, syt_validation_data, syt_test_data



#df_name_ls, df_ls = ["mimic3_clinical", "synthetic_complex"], [X_train_mimic, X_val_mimic, X_test_mimic,
          #s_time_train_mimic, s_time_val_mimic, s_time_test_mimic,
          #label_train_mimic, label_val_mimic, label_test_mimic,
          #X_train_syt, X_val_syt, X_test_syt,
          #s_time_train_syt, s_time_test_syt, s_time_val_syt,
          #label_train_syt, label_val_syt, label_test_syt]

tic = time.time()
mimic_train_data, mimic_validation_data,mimic_test_data,\
unos_train_data, unos_validation_data, unos_test_data,\
 syt_train_data, syt_validation_data, syt_test_data = __rnnDataFormat__()
toc = time.time()
print(f">>>> time elapsed: {__timefmt__(toc - tic)}")



# Unpacking the relevant datasets
x_train_m, time_train_m, label_train_m = mimic_train_data 
x_train_u, time_train_u, label_train_u = unos_train_data
x_train_st, time_train_st, label_train_st = syt_train_data
x_val_m, time_val_m, label_val_m = mimic_validation_data 
x_val_u, time_val_u, label_val_u = unos_validation_data 
x_val_st, time_val_st, label_val_st = syt_validation_data
x_test_m, time_test_m, label_test_m = mimic_test_data 
x_test_u, time_test_u, label_test_u = unos_test_data 
x_test_st, time_test_st, label_test_st = syt_test_data



print(f">>>> x_train_m shape: {x_train_m.shape}, time_train_m shape: {time_train_m.shape}, label_train_m shape: {label_train_m.shape}")

print(f">>>> x_train_u shape: {x_train_u.shape}, time_train_u shape: {time_train_u.shape}, label_train_u shape: {label_train_u.shape}")

print(f">>>> x_val_m shape: {x_val_m.shape}, time_val_m shape: {time_val_m.shape}, label_val_m shape: {label_val_m.shape}")

print(f">>>> x_val_u shape: {x_val_u.shape}, time_val_u shape: {time_val_u.shape}, label_val_u shape: {label_val_u.shape}")

print(f">>>> x_test_m shape: {x_test_m.shape}, time_test_m shape: {time_test_m.shape}, label_test_m shape: {label_test_m.shape}")

print(f">>>> x_test_u shape: {x_test_u.shape}, time_test_u shape: {time_test_u.shape}, label_test_u shape: {label_test_u.shape}")

print(f">>>> x_train_st shape: {x_train_st.shape}, time_train_st shape: {time_train_st.shape}, label_train_st shape: {label_train_st.shape}")

print(f">>>> x_val_st shape: {x_val_st.shape}, time_val_st shape: {time_val_st.shape}, label_val_st shape: {label_val_st.shape}")

print(f">>>> x_test_st shape: {x_test_st.shape}, time_test_st shape: {time_test_st.shape}, label_test_st shape: {label_test_st.shape}")



## The data loader

class MyCustomData(Dataset):
  """
  @Author: Martin Pius
  --------------------
  This class create an iterator to load our csv file in chunks
  returns
  --------
  Torch.Tensor :X with shape [Batch_size, seq_len, input_dims], Surv_time with 
                shape [Batch_size, 1], Label with shape [Batch_size, 1]
  """
  def __init__(self,X, Surv_time, Label):
    self.X =  X
    self.Surv_time = Surv_time
    self.Label = Label
  
  def __len__(self):
    return len(self.Surv_time)
  
  def __getitem__(self, idx):
    X = self.X[idx]
    Surv_time = self.Surv_time[idx]
    Label = self.Label[idx]
    return X, Surv_time, Label



def complex_surv_data_loader(x_train_m, time_train_m, label_train_m,
                             x_train_u, time_train_u, label_train_u,
                             x_train_st, time_train_st, label_train_st,
                             x_val_m, time_val_m, label_val_m,
                             x_val_u, time_val_u, label_val_u,
                             x_val_st, time_val_st, label_val_st,
                             x_test_m, time_test_m, label_test_m,
                             x_test_u, time_test_u, label_test_u,
                             x_test_st, time_test_st, label_test_st):
  """
  @Author: Martin Pius
  --------------------
  This module construct the data loaders objects ready to train our ComplexSurv model
  Arguments:
  ----------
  x_train: pd.DataFrame
  x_val: pd.DataFrame
  x_test: pd.DataFrame
  y_train: pd.DataFrame
  y_test: pd.DataFrame
  y_val: pd.DataFrame
  
  returns:
  --------
  pytorch iterators:
  train_loader
  validation_loader
  test_loader
  """
  train_data_mimic3 = MyCustomData(x_train_m, time_train_m,label_train_m)
  validation_data_mimic3 = MyCustomData(x_val_m, time_val_m,label_val_m)
  test_data_mimic3 = MyCustomData(x_test_m, time_test_m,label_test_m)
  train_loader_mimic3 = DataLoader(dataset = train_data_mimic3, shuffle = True, batch_size = 16, drop_last = True)
  validation_loader_mimic3 = DataLoader(dataset = validation_data_mimic3, shuffle = True, batch_size = 16, drop_last = True)
  test_loader_mimic3 = DataLoader(dataset = test_data_mimic3, shuffle = True, batch_size = 16, drop_last = True)

  train_data_unos = MyCustomData(x_train_u, time_train_u,label_train_u)
  validation_data_unos = MyCustomData(x_val_u, time_val_u,label_val_u)
  test_data_unos = MyCustomData(x_test_u, time_test_u,label_test_u)
  train_loader_unos = DataLoader(dataset = train_data_unos, shuffle = True, batch_size = 16, drop_last = True)
  validation_loader_unos = DataLoader(dataset = validation_data_unos, shuffle = True, batch_size = 16, drop_last = True)
  test_loader_unos = DataLoader(dataset = test_data_unos, shuffle = True, batch_size = 16, drop_last = True)

  train_data_syt = MyCustomData(x_train_st, time_train_st,label_train_st)
  validation_data_syt = MyCustomData(x_val_st, time_val_st,label_val_st)
  test_data_syt = MyCustomData(x_test_st, time_test_st,label_test_st)
  train_loader_syt = DataLoader(dataset = train_data_syt, shuffle = True, batch_size = 16, drop_last = True)
  validation_loader_syt = DataLoader(dataset = validation_data_syt, shuffle = True, batch_size = 16, drop_last = True)
  test_loader_syt = DataLoader(dataset = test_data_syt, shuffle = True, batch_size = 16, drop_last = True)
  return train_loader_mimic3, validation_loader_mimic3, test_loader_mimic3, train_loader_unos, validation_loader_unos, test_loader_unos, train_loader_syt, validation_loader_syt, test_loader_syt



## Instantiating the data loaders ready for training

train_loader_mimic3, validation_loader_mimic3, test_loader_mimic3,\
train_loader_unos, validation_loader_unos, test_loader_unos,\
train_loader_syt, validation_loader_syt, test_loader_syt \
= complex_surv_data_loader(x_train_m, time_train_m, label_train_m,
                             x_train_u, time_train_u, label_train_u,
                             x_train_st, time_train_st, label_train_st,
                             x_val_m, time_val_m, label_val_m,
                             x_val_u, time_val_u, label_val_u,
                             x_val_st, time_val_st, label_val_st,
                             x_test_m, time_test_m, label_test_m,
                             x_test_u, time_test_u, label_test_u,
                             x_test_st, time_test_st, label_test_st)



xtm, ttm, ltm = next(iter(train_loader_mimic3))
xvm, tvm, lvm = next(iter(validation_loader_mimic3))
xtem, ttem, ltem = next(iter(test_loader_mimic3))

xtu, ttu, ltu = next(iter(train_loader_unos))
xvu, tvu, lvu = next(iter(validation_loader_unos))
xteu, tteu, lteu = next(iter(test_loader_unos))

xtst, ttst, ltst = next(iter(train_loader_syt))
xvst, tvst, lvst = next(iter(validation_loader_syt))
xtest, ttest, ltest = next(iter(test_loader_syt))



print(f">>>> x_train_m_batch shape: {xtm.shape}, time_train_m_batch shape: {ttm.shape}, label_train_m_batch shape: {ltm.shape}")

print(f">>>> x_train_u_batch shape: {xtu.shape}, time_train_u_batch shape: {ttu.shape}, label_train_u_batch shape: {ltu.shape}")

print(f">>>> x_valid_m_batch shape: {xvm.shape}, time_valid_m_batch shape: {tvm.shape}, label_valid_m_batch shape: {lvm.shape}")

print(f">>>> x_valid_u_batch shape: {xvu.shape}, time_valid_u_batch shape: {tvu.shape}, label_valid_u_batch shape: {lvu.shape}")

print(f">>>> x_test_m_batch shape: {xtem.shape}, time_test_m_batch shape: {ttem.shape}, label_test_m_batch shape: {ltem.shape}")

print(f">>>> x_test_u_batch shape: {xteu.shape}, time_test_u_batch shape: {tteu.shape}, label_test_u_batch shape: {lteu.shape}")

print(f">>>> x_train_syt_batch shape: {xtst.shape}, time_train_syt_batch shape: {ttst.shape}, label_train_syt_batch shape: {ltst.shape}")

print(f">>>> x_test_syt_batch shape: {xtest.shape}, time_test_syt_batch shape: {ttest.shape}, label_test_syt_batch shape: {ltest.shape}")

print(f">>>> x_val_syt_batch shape: {xvst.shape}, time_val_syt_batch shape: {tvst.shape}, label_val_syt_batch shape: {lvst.shape}")

# Instantiating the model classes:

def __initializer__(dataset_name):
  rnn_type = ["RNN","GRU","LSTM"]
  hidden_fc = 512
  BATCH_SIZE = 16
  rnn_hidden_dim = 256
  num_events = 3
  num_layers = 2
  dropout = 0.25
  surv_time_dim = 1
  
  if dataset_name == "unos":
    max_time_horizon = 35
    seq_len = 3
    input_dim = 111
    dim_cat = 52
    dim_num = 59
    hidden_cat = 40
    hidden_num = 40
    drp = 0.5
    encoder = complex_survrnn_fin.Encoder_RNN4Surv(input_dim,
               rnn_hidden_dim,
               num_layers,
               rnn_type[2],
               True,
               dropout,
               seq_len,
               dim_num, 
               dim_cat,
               hidden_num,
               hidden_cat,
               bidirectional = True)
    rnn_output, rnn_hidden, embedding, AE_out = encoder(xtu.to(torch.float32))
    rnn_output_dim = rnn_output.shape[2]
      # instantiate the attention mechanism
    attention = complex_survrnn_fin.__AdditiveAttentionV1__(rnn_hidden_dim,rnn_output_dim)
      # sanity check for the decoder
    decoder = complex_survrnn_fin.RiskSpecificSubNetworks(hidden_fc,
                                 max_time_horizon,
                                 num_events,
                                  attention,
                                  dropout, input_dim,
                                  seq_len, rnn_output_dim)
    predictions = decoder(xtu.to(torch.float32), rnn_hidden,rnn_output)
    model = complex_survrnn_fin.ComplexSurvNetwork(encoder, decoder)
    predictions_final,AE_out  = model(xtu.to(torch.float32))
    assert predictions_final.shape == (BATCH_SIZE * num_events, num_events, max_time_horizon)
    assert AE_out.shape == (BATCH_SIZE, seq_len, input_dim)

  elif dataset_name == "mimic3":
    max_time_horizon = 173
    seq_len = 4
    input_dim = 38 
    ae_dim = 24 
    ae_embd = 24
    encoder = complexsurvrnn_msn.Encoder_RNN4Surv(input_dim, rnn_hidden_dim, num_layers, rnn_type[2], True, 0.10, 24, 24,seq_len, False)
    rnn_output, rnn_hidden, embedding, AE_out = encoder(xtm.to(torch.float32))
    rnn_output_dim = rnn_output.shape[2]
    attention = complexsurvrnn_msn.__AdditiveAttentionV1__(rnn_hidden_dim,rnn_output_dim)
    decoder = complexsurvrnn_msn.RiskSpecificSubNetworks(hidden_fc,
                                max_time_horizon,
                                num_events,
                                 attention,
                                 dropout, input_dim,
                                 rnn_output_dim,
                                  seq_len)
    predictions = decoder(xtm.to(torch.float32), rnn_hidden,rnn_output)
    model = complexsurvrnn_msn.ComplexSurvNetwork(encoder, decoder)
    predictions_final,AE_out  = model(xtm.to(torch.float32))
    assert predictions_final.shape == (BATCH_SIZE * num_events, num_events, max_time_horizon)
    assert AE_out.shape == (BATCH_SIZE, seq_len, input_dim) 

  else:
    max_time_horizon = 31
    seq_len = 4
    input_dim = 14
    ae_dim = 8
    ae_embd = 8
    encoder = complex_survrnn_fin_sim.Encoder_RNN4Surv(input_dim, rnn_hidden_dim, num_layers, rnn_type[2], True, 0.10, ae_dim, ae_embd,seq_len, False)
    rnn_output, rnn_hidden, embedding, AE_out = encoder(xtst.to(torch.float32))
    rnn_output_dim = rnn_output.shape[2]
    attention = complex_survrnn_fin_sim.__AdditiveAttentionV1__(rnn_hidden_dim,rnn_output_dim)
    decoder = complex_survrnn_fin_sim.RiskSpecificSubNetworks(hidden_fc,
                                 max_time_horizon,
                                 num_events,
                                 attention,
                                 dropout, input_dim,
                                 rnn_output_dim)
    predictions = decoder(xtst.to(torch.float32), rnn_hidden,rnn_output)
    model = complex_survrnn_fin_sim.ComplexSurvNetwork(encoder, decoder)
    predictions_final,AE_out  = model(xtst.to(torch.float32))
    assert predictions_final.shape == (BATCH_SIZE * num_events, num_events, max_time_horizon)
    assert AE_out.shape == (BATCH_SIZE, seq_len, input_dim)
  return model

