# -*- coding: utf-8 -*-
"""RiskSpecificSubNets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AHa99a0DPuso-Y5S7BjyW1bgdgfSZ0xC
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pytorch-lightning

from torch import nn, mm
import pytorch_lightning as pl
from argparse import ArgumentTypeError
import torch, time
# from Modules.complex_surv_attention import __AdditiveAttentionV1__
# from Module.rnn_complex_surv_lightning import Encoder_RNN4Surv

# The top architecture consists of subnetworks of FC-Layers to predict survival time
# Risk-specific sub-networks (for each competing risk) takes the Context which is the 
# point-wise multiplication of RIW and original input tensor) produce the outputs 
# Predictions ==> Survival time distribution for the Risk-Specific-Networks
# Predictions shape is reshaped into ==> [batch_size, num_events, max_time_horizon]
class RiskSpecificSubNetworks(pl.LightningModule):
  def __init__(self, hidden_dim, 
               max_time_horizon,
               num_events, 
               attention, 
               dropout,
               input_dim,
               rnn_output_dim,
               surv_time_dim):
    """
    hidden_dim: ==> hidden dimension for each FC-layers subnetwork
    max_time_horizon: ==> maximum survival time accross all competing events
    num_events: total number of competing events. 
    """
    super().__init__()
    self.input_dim = input_dim
    self.surv_time_dim = surv_time_dim
    self.rnn_output_dim = rnn_output_dim
    self.attention = attention
    self.num_events = num_events
    self.max_time_horizon = max_time_horizon
    # Container to hold nn-layers
    self.fc_list = nn.ModuleList([
        nn.Sequential(
          nn.Linear(self.input_dim + self.rnn_output_dim + self.surv_time_dim, 2*hidden_dim),
          nn.ReLU(),
          nn.Dropout(dropout),
          nn.Linear(2*hidden_dim, hidden_dim),
          nn.Linear(hidden_dim, self.num_events * self.max_time_horizon),
          nn.Softmax(dim = 1)
        )
        for _ in range(self.num_events)])
    
  
  def forward(self, x, rnn_hidden, surv_time, rnn_output):
    '''
    surv_time == survival time: shape == [batch_size, 1]
    x : rnn_input: shape == [batch, input_dim]
    rnn_output:shape == [seq_len, batch_size, output_dim]
    rnn_hidden: shape == [batch_size, hidden_dim]
    ### we need to build a context vector using the (RIW) ==> attention ####
    Note: RIW ==> attention: shape ==> [batch_size, seq_len]

    '''
    #torch.autograd.set_detect_anomaly(True) # for anomaly detection
    seq_len = rnn_output.shape[0]
    p = self.attention(rnn_hidden, rnn_output) # shape == [batch_size, seq_len]
    p = p.unsqueeze(1) # shape == [batch_size, 1, seq_len]
    rnn_out1 = rnn_output.permute(1,0,2) # new shape == [batch_size, seq_len, hidden*2]
    context = torch.bmm(p, rnn_out1) # shape == [1, batch_size, hidden *2]
    #context = torch.zeros(p.shape[0], p.shape[1], rnn_out1.shape[2]) # shape == [batch_size, 1, hidden_dim *2]
    #for i in range(rnn_out1.shape[0],2):
      #context[i] = torch.stack([rnn_out1[i] @ p[i], rnn_out1[i+1] @ p[i+1]])
    
    context = context.squeeze(1) # shape == [batch_size, hidden_dim*2]
    rsm_input = torch.cat((context, x, surv_time), dim = 1)
    
    #rsm_input = torch.cat((x.unsqueeze(1).repeat(1,seq_len,1), context), dim = 2) # shape == [batch_size, 1, hidden *2 + input_dim]
    #rsm_input = rsm_input.view(rsm_input.shape[0], -1) # shape == [batch_size, hidden*2 + input_dim]
    a = []
    for i in range(self.num_events):
      a.append(rsm_input)
    predictions = torch.cat([fc(x) for x, fc in zip(a, self.fc_list)], dim = 1)
    return predictions.view(-1, self.num_events, self.max_time_horizon)
    #return context, rsm_input


# Sanity check, the encoder -----* ------* -------* ---------* ---------* --------* testing:
hidden_fc = 512
max_time_horizon = 100
num_events = 5
batch_size = 32
seq_len = 5
dropout = 0.25
input_dim = 100
surv_time_dim = 1
rnn_hidden_dim = 256
num_layers = 2 
surv_time_dim = 1
rnn_type = ["RNN", "GRU", "LSTM"]
rnn_input = torch.randn(size = (batch_size, input_dim))

# Running the encoder network 
rnn_surv = Encoder_RNN4Surv(input_dim = input_dim,
                    hidden_dim = rnn_hidden_dim,
                    num_layers = num_layers,
                    seq_len = seq_len,
                    rnn_type = rnn_type[0],
                    bidirectional = True)

rnn_output, rnn_hidden = rnn_surv(rnn_input)

# Risk-specific probabilities (attention)
attention = __AdditiveAttentionV1__(rnn_hidden.shape[1], rnn_output.shape[2], bidirectional = True)

decoder = RiskSpecificSubNetworks(hidden_fc,
                                  max_time_horizon,
                                  num_events,
                                  attention,
                                  dropout,
                                  input_dim,
                                  rnn_output.shape[2],
                                  surv_time_dim)
# Display the model's graph
print(decoder)
surv_time = torch.rand(size = (batch_size, surv_time_dim))
preds = decoder(rnn_input, rnn_hidden, surv_time, rnn_output)

assert preds.shape == (batch_size * num_events, num_events, max_time_horizon)




