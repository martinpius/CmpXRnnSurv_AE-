# -*- coding: utf-8 -*-
"""Complex_Surv Data-Loader

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d9sFnfOUBVo7-sJgal0VuFit2wQpzMZK
"""

from google.colab import drive, files, auth
drive.mount("/content/drive/", force_remount = True)
try:
  COLAB = True
  import pandas as pd
  import numpy as np
  import torch, time
  from collections import Counter
  from sklearn.preprocessing import LabelEncoder, MinMaxScaler
  import matplotlib.pyplot as plt
  import torch
  pd.set_option("display.max_columns", None)
  pd.set_option("display.max_rows", None)
  print(f">>>> You are on CoLaB with Pandas version {pd.__version__}")

  def __timefmt__(t: float = 123.178)->float:
    h = int(t / (60 * 60))
    m = int(t % (60 * 60) / 60)
    s = int(t % 60)
    return f"hrs: {h} min: {m:>02} sec: {s:>05.2f}"

except Exception as e:
  print(f">>>> {type(e)}: {e}\n>>>> Please corect {type(e)} and reload the drive")
  COLAB = False
print(f">>>> testing the time formating function...........\n>>>> time elapsed\t{__timefmt__()}")

files.upload()

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pytorch-lightning

import complexsurvehelper

#auth.authenticate_user()
#%%bigquery --project strange-song-274510 MIMIC3_CLEAN
#SELECT *
#FROM `strange-song-274510.111.MIMIC3_CLEAN_MARTIN`

def df_to_tensor(df):
  return torch.from_numpy(df.values).to(torch.int64)

# Loading the data from the drive
def __readmyCSV__(data_path,fnames, chunksize = 10000):
  """
  @Author: Martin Pius
  --------------------
  -This method read the big CSV datafiles from the given path
  Parameters:
  -----------
  data_path: str: A path/URL contains the data files
  fnames: List: List of files 
  chunk_size: Int: A chunk size 

  Returns:
  ----------
  ICU_data: pd.DataFrame 
  """
  for k in range(len(fnames)):
    dfm = data_path +"/" + fnames[k] +".csv"
    icu_data = pd.read_csv(dfm, chunksize = chunksize)
    if fnames[k]== 'icu_clean1':
      ICU_data1 = pd.concat(icu_data)
    elif fnames[k] == "icu_clean2":
      ICU_data2 = pd.concat(icu_data)
    elif fnames[k] == "MIMIC3_CLEAN_MARTIN":
      ICU_data_clean = pd.concat(icu_data)
    else:
      ICU_data3 = pd.concat(icu_data)
  frames = [ICU_data1, ICU_data2, ICU_data3]
  return ICU_data_clean

data_path = "/content/drive/MyDrive/MIMIC3_BIG_QUERY"

fnames = ['icu_clean1', 'icu_clean2', 'icu_clean3', "MIMIC3_CLEAN_MARTIN"]



def MIMICIII_preprocess(data_path, files_names):
  """
  @Author: Martin Pius
  ---------------------
  -This Module pre-processing the mimic3.csv data file and return 
   the torch tensor dataset


  """
  ICU_data = __readmyCSV__(data_path = data_path, fnames = files_names)
  
  #drop columns with excessive missing values
  for col in list(ICU_data.columns):
    if ICU_data[col].isna().sum()>60000:
      ICU_data.drop(col, axis = 1, inplace = True) 
  
  #Impute the missing values [for categorical add one class "NA"]
  for col in list(ICU_data.columns):
    if ICU_data.dtypes[col] == "object":
      ICU_data[col] = ICU_data[col].fillna("NA")
    else:
      ICU_data[col] = ICU_data[col].fillna(ICU_data[col].mean())
  
  # prepares the target variable for recurrent events with competing risks
  target = ICU_data.loc[:, ["los_icu","DIAGNOSIS","hospital_expire_flag"]]
  target["label"] = np.zeros(shape = (len(target),), dtype = np.int64)

  for k in range(len(target)):
    if target["DIAGNOSIS"][k] == 1 and target["hospital_expire_flag"][k] == 0:
      target["label"][k] = "heart problem"
    elif target["DIAGNOSIS"][k] == 0 and target["hospital_expire_flag"][k] == 0:
      target["label"][k] = "other risks"
    else:
      target["label"][k] = "censore"

  label = LabelEncoder().fit_transform(target["label"])
  label_dict =  {"heart problem": 1, "other risks": 2, "censore": 0}
  surv_time = target['los_icu']
  surv_time_dict = Counter(surv_time)
  keys = np.array(list(surv_time_dict.keys()))
  values = np.array(list(surv_time_dict.values()))
  
  #Visualizing the distribution of survival time
  x, y = list(), list()
  for i in range(50):
    if values[i] > 9000:
      x.append(keys[i]), y.append(values[i])
  
  #Get the covariates matrix
  covariates = ICU_data.drop(["los_icu","DIAGNOSIS","hospital_expire_flag", 
                              "intime", "outtime","icustay_id","subject_id",
                              "hadm_id", "admittime","dischtime","depression", 
                              "alcohol_abuse", "blood_loss_anemia","cardiac_arrhythmias", 
                              "deficiency_anemias","drug_abuse","paralysis", 
                              "paralysis_1", "hypothyroidism","psychoses"], axis = 1)

  #Categorical embedding of categorical variables
  for col in list(covariates.columns):
    if covariates[col].dtypes == "object":
      covariates[col] = LabelEncoder().fit_transform(covariates[col])

  for col in list(covariates.columns):
    if col == "gender" or col == "ethnicity_grouped":
      covariates[col] = covariates[col].astype("category")

  #Spliting the data matrix into numeric and categorical features
  cat_features , num_features = [], []
  for col in list(covariates.columns):
    if covariates[col].dtypes == "category":
      cat_features.append(col)
    else:
      num_features.append(col)

  cat_features = num_features[:16] +num_features[18:27]  + cat_features
  num_features = [num_features[16]] + [num_features[17]] + num_features[27:]

  assert len(list(covariates.columns)) == len(cat_features + num_features)

  for col in list(covariates.columns):
    if col in cat_features:
      covariates[col] = covariates[col].astype("category")
  
  # get the embedding sizes for each categorical variable
  embedded_cols = {n: len(col.cat.categories) for n,col in covariates[cat_features].items() if len(col.cat.categories) > 0}
  embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]

  # Prepare the data matrix as torch tensor
  dfm = pd.concat([covariates.loc[:, cat_features], covariates.loc[:, num_features]], axis = 1)
  dfm = df_to_tensor(dfm)

  return (label, label_dict),(surv_time, surv_time_dict), (x, y), (cat_features, num_features), (embedding_sizes, dfm)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# tic = time.time()
# (label, label_dict),(surv_time, surv_time_dict), (x, y), (cat_features, num_features), (embedding_sizes, dfm) = MIMICIII_preprocess(data_path, fnames)
# toc = time.time()
#

print(f">>>> time elapsed: {__timefmt__(toc - tic)}")

display(dfm[-16:, :])



# Visualize the distribution of patients based on ICU stays
fig, ax = plt.subplots(figsize = (8,6))
ax.bar(x,y, width = 2)
ax.ticklabel_format(useOffset=False, style='plain')
plt.show()

#### Sanity check for the data preprocessing class #####

fc_lyrs = [256, 128, 64]
num_numeric = len(num_features)

prep_features = complexsurvehelper.Features_Prep(embedding_sizes, num_numeric, fc_lyrs)

rnn_input = prep_features(dfm)

rnn_input.shape

rnn_input[:10, :]